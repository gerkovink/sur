# Geavanceerde data technieken {#sec-mt6}

::: callout-tip
## Hoorcollege 10 juni 2025
Vandaag gaan we zelf data maken. Op het eerste gezicht klinkt dat misschien een beetje frauduleus - hoe komen we aan die data en waarom zou deze data goed zijn? We zullen zien dat we data-gebaseerde en informatie-gebaseerde modellen kunnen gebruiken om valide inferentie (gevolgtrekkingen) te kunnen trekken op basis van bestaande - al dan niet compleet geobserveerde - data. We gebruiken daarvoor veel van de technieken en theorie die we tot nu toe hebben geleerd. Eigenlijk komt alles wat we tot nu toe hebben besproken samen in dit college. De collegeslides kunt u [hier vinden](slides/lec-6.html)
:::

---
subtitle: "A very quick crash course in `mice`"

author: 
  - name: Gerko Vink
    orcid: 0000-0001-9767-1924
    email: g.vink@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
  - name: Hanne Oberman
    orcid: 0000-0003-3276-2141
    email: h.i.oberman@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
format:
  html:
    highlight-style: github
    number-sections: false
    toc: true
    toc-depth: 3
---

[![](https://zenodo.org/badge/271731959.svg)](https://zenodo.org/badge/latestdoi/271731959)

The aim of this guide is to make you familiar with [mice](https://github.com/amices/mice) in `R` and to enhance your understanding of multiple imputation, in general. You will learn how to perform and inspect multiple imputations and how to pool the results of analyses performed on multiply-imputed data, how to approach different types of data and how to avoid some of the pitfalls many data scientists may fall into. The main objective is to increase your knowledge and understanding on applications of multiple imputation. 

We start by loading (with `library()`) the necessary packages and fixing the random seed to allow for our outcomes to be replicable. 
```{r, message=FALSE, warning=FALSE}
library(mice)    # data imputation
library(ggmice)  # plotting 
library(ggplot2) # plotting 
library(dplyr)   # data manipulation
library(purrr)   # functional programming
```


---

# Multiple imputation with `mice`

---

We fix the RNG seed to allow for replication of the below results. 
```{r}
set.seed(123)
```

The `mice` package contains several datasets. Once the package is loaded, these datasets can be used. Have a look at the `nhanes` dataset (Schafer, 1997, Table 6.14) by typing
```{r}
nhanes
```
 The `nhanes` dataset is a small data set with non-monotone
 missing values. It contains 25 observations on four variables:
 *age group*, *body mass index*, *hypertension*
 and *cholesterol (mg/dL)*.

To learn more about the data, use one of the two following help commands:
```{r, eval = FALSE, cache = FALSE}
help(nhanes)
?nhanes
```

The `nhanes` dataset is incomplete. We can visualize the missing data patterns by
```{r}
md.pattern(nhanes)
```

For more informative axis labels, we can use the equivalent `ggmice` function `plot_pattern()` as follows
```{r}
plot_pattern(nhanes)
```

Although the most common pattern is the one where all variables are observed, the majority of cases have at least one missing value.

---

**1. Vary the number of imputations, such that the `nhanes` set is imputed $m=3$ times. **

The number of imputed data sets can be specified by the `m = ...` argument. For example, to create just three imputed data sets, specify
```{r, cache = FALSE}
imp <- mice(nhanes, m = 3, print = FALSE)
```

The `print = FALSE` argument omits printing of the iteration history from the output. The main reason to omit printing here is to save space in the document. 

---

**2. Change the predictor matrix**

The predictor matrix is a square matrix that specifies the variables that can be used to impute each incomplete variable. Let us have a look at the predictor matrix that was used
```{r, cache = FALSE}
imp$pred
```
Each variable in the data has a row and a column in the predictor matrix. A value `1` indicates that the column variable was used to impute the row variable. For example, the `1` at entry `[bmi, age]` indicates that variable `age` was used to impute the incomplete variable `bmi`. Note that the diagonal is zero because a variable is not allowed to impute itself. The row of `age` is redundant, because there were no missing values in `age`. Even though predictor relations are specified for `age`, `mice` will not use these relations because it will never overwrite the observed values with imputations. `mice` gives you complete control over the predictor matrix, enabling you to choose your own predictor relations. This can be very useful, for example, when you have many variables or when you have clear ideas or prior knowledge about relations in the data at hand. 

There are two ways in which you can create a predictor matrix in `mice`:

- A. You can grab the `predictorMatrix` from every object returned by `mice()`:

For example, we can use any `mice()` object fitted to the data to grab the `predictorMatrix` or we can use mice to quickly initialize a predictor matrix, and change it afterwards, without running the algorithm. This latter approach can be done by setting the maximum number of iterations to `maxit=0`. This leaves the algorithm at initialization, but generates the necessary inner objects. 
```{r, cache = FALSE}
ini <- mice(nhanes, maxit = 0, print = FALSE)
pred <- ini$pred
pred
```
The object `pred` contains the predictor matrix from an initial run of `mice` with zero iterations. It is a square matrix that captures the information about which variables (in the rows) are imputed based on which predictors (in the columns). 

- B. We can use `make.predictorMatrix()` to generate a predictor matrix from any incomplete data set.

For example, 
```{r}
pred <- make.predictorMatrix(nhanes)
pred
```

Altering the predictor matrix and returning it to the mice algorithm is very simple. For example, the following code removes the variable `hyp` from the set of predictors, but still leaves it to be predicted by the other variables.
```{r, cache = FALSE}
pred[, "hyp"] <- 0
pred
```

Use your new predictor matrix in `mice()` as follows
```{r, cache = FALSE}
imp <- mice(nhanes, predictorMatrix =  pred, print = FALSE)
```

As you can see, we can easily feed the new matrix `pred` to `mice`. We can also abbreviate the logical operator in the argument `print = FALSEALSE`. 

There is a `quickpred()` function that applies a quick selection procedure of predictors, which can be handy for datasets containing many variables. See `?quickpred` for more info. Selecting predictors according to data relations with a minimum correlation of $\rho=.30$ can be done by
```{r, cache = FALSE}
ini <- mice(nhanes, pred = quickpred(nhanes, mincor = .3), print = FALSE)
ini$pred
```
For large predictor matrices, it can be useful to export them to dedicated spreadsheet software like e.g. Microsoft Excel for easier configuration (e.g. see the [`xlsx` package](https://cran.r-project.org/web/packages/xlsx/index.html) for easy exporting and importing of Excel files). Importing data is straightforward in `RStudio` through `File` > `Import Dataset`. 

You can visualize the predictor matrix with the `ggmice` function `plot_pred()` as follows
```{r}
plot_pred(pred)
```

---

**3. Inspect the convergence of the algorithm**

The `mice()` function implements an iterative Markov Chain Monte Carlo type of algorithm. Let us have a look at the trace lines generated by the algorithm to study convergence:
```{r, cache = FALSE}
imp <- mice(nhanes, print = FALSE)
plot_trace(imp)
```

The plot shows the mean (left) and standard deviation (right) of the imputed values only. In general, we would like the streams to intermingle (mixing) and be free of any trends at the later iterations (non-stationary). We inspect trends for the imputed values alone, because the observed data does not change. In our case we cannot speak of convergence, especially not for `bmi`. More iterations or a different model are needed. 

The `mice` algorithm uses random sampling, and therefore, the results will be (perhaps slightly) different if we repeat the imputations with different seeds. In order to get identical `mice` objects between calls, we can fix the  use the `seed` argument.  
```{r, cache = FALSE}
imp <- mice(nhanes, seed = 123, print = FALSE)
```
where `123` is some arbitrary number that you can choose yourself. Rerunning this command will always yields the same imputed values.

---

**4. Change the imputation method**

For each column, the algorithm requires a specification of the imputation method. To see which method was used by default:
```{r, cache = FALSE}
imp$meth
```
The variable `age` is complete and therefore not imputed, denoted by the `""` empty string. The other variables have method `pmm`, which stands for *predictive mean matching*, the default in
`mice` for numerical and integer data.  

In reality, the `nhanes` data are better described a as mix of numerical and categorical data. Let us take a look at the `nhanes2` data frame:
```{r, cache = FALSE}
summary(nhanes2)
```
and the structure of the data frame
```{r, cache = FALSE}
str(nhanes2)
```
Variable `age` consists of 3 age categories, while variable `hyp` is binary. The `mice()` function takes these properties automatically into account. Impute the `nhanes2` dataset
```{r, cache = FALSE}
imp <- mice(nhanes2, print = FALSE)
imp$meth
```
Notice that `mice` has set the imputation method for
variable `hyp` to `logreg`, which implements multiple imputation by *logistic
regression*. 

An up-to-date overview of the methods in mice can be found by
```{r, warning=FALSE}
methods(mice)
```

Let us change the imputation method for `bmi` to Bayesian normal linear regression imputation
```{r, cache = FALSE}
meth <- make.method(nhanes2)
meth
meth["bmi"] <- "norm"
meth
```
The new methods vector can be visualized with the predictor matrix by
```{r}
plot_pred(pred, method = meth)
```

Now, we can run the imputations again.
```{r, cache = FALSE}
imp <- mice(nhanes2, meth = meth, print = FALSE)
```
and we may again plot trace lines to study convergence
```{r, cache = FALSE}
plot_trace(imp)
```

---

**5. Extend the number of iterations**

Though using just five iterations (the default) often works well in practice, we need to extend the number of iterations of the `mice` algorithm to confirm that there is no trend and that the trace lines intermingle well. We can increase the number of iterations to 40 by running 35 additional iterations using the `mice.mids()` function. 
```{r, cache = FALSE}
imp40 <- mice.mids(imp, maxit = 35, print = FALSE)
plot_trace(imp40)
```

---

**6. Further diagnostic checking.**

Generally, one would prefer for the imputed data to be plausible values, i.e. values that could have been observed if they had not been missing. In order to form an idea about plausibility, one may check the imputations and compare them against the observed values. If we are willing to assume that the data are missing completely at random (MCAR), then the imputations should have the same distribution as the observed data. In general, distributions may be different because the missing data are MAR (or even MNAR). However, very large discrepancies need to be screened. Let us plot the observed and imputed data of `chl` by
```{r, cache = FALSE}
ggmice(imp, aes(x = .imp, y = chl)) + 
  geom_jitter(width = .1)
```

The convention is to plot observed data in blue and the imputed data in red. The figure graphs the data values of `chl` before and after imputation. Since the PMM method draws imputations from the observed data, imputed values have the same gaps as in the observed data, and are always within the range of the observed data. The figure indicates that the distributions of the imputed and the observed values are similar. The observed data have a particular feature that, for some reason, the data cluster around the value of 187. The imputations reflect this feature, and are close to the data. Under MCAR, univariate distributions of the observed and imputed data are expected to be identical. Under MAR, they can be different, both in location and spread, but their multivariate distribution is assumed to be identical. There are many other ways to look at the imputed data.

The following commands create a the graph from the previous step for `bmi`. 
```{r, cache = FALSE}
purrr::map(names(imp$data), ~{
  ggmice(imp, aes(x = .imp, y = .data[[.x]])) + 
    geom_jitter(width = .1)
})
```

Remember that `bmi` was imputed by Bayesian linear regression and (the range of) imputed values may therefore be different than observed values. 

---

# Repeated analysis in `mice` #1

---

**7. Perform the following regression analysis on the multiply imputed data and assign the result to object `fit`. **


$$\text{bmi} = \beta_0 + \beta_1 \text{chl} + \epsilon$$

Let's run the above model on the imputed data set. 
```{r, cache = FALSE}
fit <- with(imp, lm(bmi ~ chl))
fit
```

The `fit` object contains the regression summaries for each data set. 
The new object `fit` is actually of class `mira` (*multiply imputed repeated analyses*).
```{r, cache = FALSE}
class(fit)
```
Use the `ls()` function to what out what is in the object. 
```{r, cache = FALSE}
ls(fit)
```
Suppose we want to find the regression model fitted to the
second imputed data set. It can be found as
```{r, cache = FALSE}
summary(fit$analyses[[2]])
```

---

**8. Pool the analyses from object `fit`. **

Pooling the repeated regression analyses can be done simply by typing
```{r, cache = FALSE}
pool.fit <- pool(fit)
summary(pool.fit)
pool.fit
```
which gives the relevant pooled regression coefficients and
parameters, as well as the fraction of information about the
coefficients missing due to non-response (`fmi`) and the proportion of the variation attributable to the missing data (`lambda`). The pooled fit object is of class `mipo`, which stands for *multiply imputed pooled object*. 

Alternatively, we could use a functional programming pipe to achieve the same
```{r}
fit <- imp |> 
  complete("all") |> # list where each listed element is a completed set
  map(lm, formula = bmi ~ chl) |> 
  pool() |> 
  summary()
```
Have a look at the different workflows that can be adopted with mice in [this chapter in Van Buuren's book](https://stefvanbuuren.name/fimd/workflow.html).

`mice` can to pool many analyses from a variety of packages for you (it uses `broom` to gather all parameters). For flexibility and in order to run custom pooling functions, mice also incorporates a function `pool.scalar()` which pools univariate estimates of $m$ repeated complete data analysis conform Rubin's pooling rules (Rubin, 1987, paragraph 3.1) 

---

# The `boys` data set

---

**9. The `boys` dataset is part of `mice`. It is a subset of a large Dutch dataset containing growth measures from the Fourth Dutch Growth Study. Inspect the help for `boys` dataset and make yourself familiar with its contents.**

To learn more about the contents of the data, use one of the two following help commands:
```{r eval=FALSE}
help(boys)
?boys
```

---

**10. Get an overview of the data. Find information about the size of the data, the variables measured and the amount of missingness.**

The first 10 cases are:
```{r}
head(boys, n = 10)
```
The last 10 cases are:
```{r}
tail(boys, n = 10)
```
We now have a clear indication that the data are sorted. A simple evaluation
```{r}
!is.unsorted(boys$age)
```
confirms this - `!is.unsorted()` evaluates the complement of `is.unsorted()`, so it tests whether the data are sorted. There is no `is.sorted` function in `R`. 

The dimensions of the `boys` data set are:
```{r}
dim(boys)
```
We see that the `boys` data set has 748 cases over 9 variables. From those 9 variables
```{r}
summary(boys)
```
function `summary()` informs us that testicular volume `tv` has the most missings, followed by the genital and pubic hair stages `gen` and `phb`, each with 503 missing cells.

---

**11. As we have seen before, the function `plot_pattern()` can be used to display all different missing data patterns. How many different missing data patterns are present in the boys dataframe and which pattern occurs most frequently in the data?**
```{r}
plot_pattern(boys)
```
There are 13 patterns in total, with the pattern where `gen`, `phb` and `tv` are missing occuring the most.

---

**12. How many patterns occur for which the variable `gen` (genital Tannerstage) is missing?**
```{r}
mpat <- md.pattern(boys, plot = FALSE)
sum(mpat[, "gen"] == 0)
```
Answer: 8 patterns (503 cases)

---

**13. Let us focus more precisely on the missing data patterns. Does the missing data of `gen` depend on `age`? One could for example check this by making a histogram of `age` separately for the cases with known genital stages and for cases with missing genital stages.**

To create said histogram in `R`, a missingness indicator for `gen` has to be created. A missingness indicator is a dummy variable with value `1` for observed values (in this case genital status) and `0` for missing values. Create a missingness indicator for `gen` by typing
```{r}
# use ! to indicate the opposite of NA
R <- !is.na(boys$gen) # TRUE for observed, FALSE for missing
head(R, n = 100)
tail(R, n = 100)
length(R)
```
As we can see, the missingness indicator tells us for each of the 748 values in `gen` whether it is missing (`TRUE`) or observed (`FALSE`).

A histogram can be made with 
```{r}
ggmice(boys, aes(gen)) + 
  geom_bar(fill = "white")
```


The code for a conditional histogram of `age` given `R` is
```{r}
ggmice(cbind(boys, R), aes(age)) + 
  geom_histogram(fill = "white") + 
  facet_wrap(~R)
```

The histogram shows that the missingness in `gen` is not equally distributed across `age`; or, equivalently, `age` seems to be differently distributed for observed and missing `gen`.

---

**14. Impute the `boys` dataset with mice using all default settings and name the `mids` (multiply imputed data set) object `imp`.**
```{r}
imp <- mice(boys, print = FALSE)
```

---

**15. Compare the means of the imputed data with the means of the incomplete data.**
First, we calculate the observed data means:
```{r}
boys |>
  select(-phb, -gen, -reg) |>
  colMeans(na.rm = TRUE)
```
and then the means for the $m$ imputed sets:
```{r}
imp |>
  mice::complete("all") |>
  map(select, -phb, -gen, -reg) |>  
  map(colMeans)
```
Most means are roughly equal, except the mean of `tv`, which is much lower in the imputed data sets, when compared to the incomplete data. This makes sense because most genital measures are unobserved for the lower ages. When imputing these values, the means should decrease.

Investigating univariate properties by using functions such as `summary()`, may not be ideal in the case of hundreds of variables. To extract just the information you need, for all imputed datasets, we can make use of the `with()` function. To obtain summaries for each imputed `tv` only, type
```{r warning=FALSE}
imp |>
  with(summary(tv)) |>
  summary()
```

And to obtain e.g. the means alone, run
```{r warning=FALSE}
imp |>
  with(mean(tv)) |>
  summary()
```

---

# Repeated analysis in `mice` #2

**16. Calculate a correlation between all continuous variables for the imputed `boys` data**

There are two ways in which we can calculate the correlation on the imputed data:

- **The wrong way: calculate an estimate over the *average imputed dataset* **.

Quite often people are suggesting that using the average imputed dataset - so taking the average over the imputed data set such that any realized cell depicts the average over the corresponding data in the imputed data - would be efficient and conform Rubin's rules. This is not true. Doing this will yield false inference. 

To demonstrate this, let's create the averaged data set and exclude the non-numerical columns:
```{r warning=FALSE}
ave <- imp |>
  mice::complete("long") |>
  group_by(.id) |>
  summarise_all(.funs = mean) |>
  select(-.id, -.imp, -phb, -gen, -reg)

head(ave)
```
If we now calculate Pearson's correlation, rounded to two digits:
```{r}
cor.wrong <- ave |>
  cor() |>
  round(digits = 2)
```
we obtain:
```{r}
cor.wrong
```


- **The correct way: calculate an estimate for each imputed dataset and average over the estimates**

It is best to do a [Fisher transformation](https://en.wikipedia.org/wiki/Fisher_transformation) before pooling the correlation estimates - and a backtransformation afterwards. Therefore we define the following two functions that allow us to transform and backtransform any value:
```{r}
fisher.trans <- function(x) 1/2 * log((1 + x) / (1 - x))
fisher.backtrans <- function(x) (exp(2 * x) - 1) / (exp(2 * x) + 1)
```

Now, to calculate the correlation on the imputed data
```{r}
cor <- imp |>
  mice::complete("all") |>
  map(select, -phb, -gen, -reg) |>  
  map(stats::cor) |>
  map(fisher.trans)
cor
```

The object `cor` is a list over the $m$ imputations where each listed index is a correlation `matrix`. To calculate the average over the correlation matrices, we can add the $m$ listed indices and divide them by $m$:
```{r}
cor.rect <- Reduce("+", cor) / length(cor) # m is equal to the length of the list
cor.rect <- fisher.backtrans(cor.rect)
```

If we compare the wrong estimates in `cor.wrong`
```{r}
cor.wrong
```
with the correct estimates in `cor.rect`
```{r}
round(cor.rect, digits = 2)
```

We see that the wrong estimates in `cor.wrong` have the tendency to overestimate the correlation coefficient that is correctly combined following Rubin's rules. 

The correct estimates have a diagonal of `NaN`'s, because the tranformation of a correlation of `1` yields `Inf` and the backtransformation of `Inf` has no representation in real number space. We know the diagonal is supposed to be 1, so we can simply correct this
```{r}
diag(cor.rect) <- 1
cor.rect
```


::: {.callout-tip}

## Why does the average data set not serve as a good basis for analysis?

In [`FIMD v2`, paragraph 5.1.2](https://stefvanbuuren.name/fimd/workflow.html) Stef mentions the following:

*The average workflow is faster and easier than the correct methods, since there is no need to replicate the analyses $m$ times. In the words of Dempster and Rubin (1983), this workflow is*

***seductive because it can lull the user into the pleasurable state of believing that the data are complete after all.***

*The ensuing statistical analysis does not know which data are observed and which are missing, and treats all data values as real, which will underestimate the uncertainty of the parameters. The reported standard errors and  p-values after data-averaging are generally too low. The correlations between the variables of the averaged data will be too high. For example, the correlation matrix in the average data
are more extreme than the average of the $m$ correlation matrices, which is an example of ecological fallacy. As researchers tend to like low  p-values and high correlations, there is a cynical reward for the analysis of the average data. However, analysis of the average data cannot give a fair representation of the uncertainties associated with the underlying data, and hence is not recommended.*


So, please stay away from averaging the imputed data sets. Instead, use the correct workflow of analyzing the imputed sets seperately and combining the inference afterwards.

:::


# The importance of the imputation model

The `mammalsleep` dataset is part of `mice`. It contains the Allison and Cicchetti (1976) data for mammalian species. To learn more about this data, type
```{r eval=FALSE}
help(mammalsleep)
```

---

**17. Get an overview of the data.** 

Find information about the size of the data, the variables measured and the amount of missingness.
```{r}
head(mammalsleep)
summary(mammalsleep)
str(mammalsleep)
```
As we have seen before, the function `plot_pattern()` can be used to display all different missing data patterns. How many different missing data patterns are present in the `mammalsleep` dataframe and which pattern occurs most frequently in the data? 
```{r}
plot_pattern(mammalsleep)
```
Answer: 8 patterns in total, with the pattern where everything is observed occuring the most (42 times).

---

**18. Generate five imputed datasets with the default method `pmm`. Give the algorithm 10 iterations. **
```{r}
imp1 <- mice(mammalsleep, maxit = 10, print = FALSE, seed = 123)
```
We ignore the `loggedEvents` for now: it contains a list of all decisions and exclusions that are performed by the `mice` algorithm. To inspect the trace lines for assessing algorithmic convergence:
```{r}
plot_trace(imp1)
```

---

**19. Perform a regression analysis on the imputed dataset with `sws` as dependent variable and `log10(bw)` and `odi` as independent variables.**

```{r}
fit1 <- with(imp1, lm(sws ~ log10(bw) + odi))
```

---

**20. Pool the regression analysis and inspect the pooled analysis.**
```{r}
est1 <- pool(fit1)
est1
summary(est1)
```

The `fmi` and `lambda` are much too high. This is due to `species` being included in the imputation model. Because there are 62 species and mice automatically converts factors (categorical variables) to dummy variables, each species is modeled by its own imputation model. 

---

**21. Impute `mammalsleep` again, but now exclude `species` from the data.**
```{r, cache = FALSE}
imp2 <- mice(mammalsleep[ , -1], maxit = 10, print = FALSE, seed = 123)
```

---

**22. Compute and pool the regression analysis again. **
```{r}
fit2 <- with(imp2, lm(sws ~ log10(bw) + odi))
est2 <- pool(fit2)
est2
summary(est2)
```

Note that the `fmi` and `lambda` have dramatically decreased. The imputation model has been greatly improved. 

---

**23. Plot the trace lines for the new imputations**
```{r}
plot_trace(imp2)
```

Even though the fraction of information missing due to nonresponse (fmi) and the relative increase in variance due to nonresponse (lambda) are nice and low, the convergence turns out to be a real problem. The reason is the structure in the data. Total sleep (`ts`) is the sum of paradoxical sleep (`ps`) and short wave sleep (`sws`). This relation is ignored in the imputations, but it is necessary to take this relation into account. `mice` offers a routine called *passive imputation*, which allows users to take transformations, combinations and recoded variables into account when imputing their data. 

---

# Passive Imputation

There is often a need for transformed, combined or recoded versions of the data. In the case of incomplete data, one could impute the original, and transform the completed original afterwards, or transform the incomplete original and impute the transformed version. IFALSE, however, both the original and the transformed version are needed within the imputation algorithm, neither of these approaches work: One cannot be sure that the transformation holds between the imputed values of the original and transformed versions. `mice` has a built-in approach, called *passive imputation*, to deal with situations as described above. The goal of passive imputation is to maintain the consistency among different transformations of the same data. As an example, consider the following deterministic function in the `boys` data
\[\text{BMI} = \frac{\text{Weight (kg)}}{\text{Height}^2 \text{(m)}}\]
or the compositional relation in the mammalsleep data:
\[\text{ts} = \text{ps}+\text{sws}\]

---

**24. Use passive imputation to impute the deterministic sleep relation in the `mammalsleep` data. Name the new multiply imputed dataset `pas.imp`.**

First, we create a `method` vector:
```{r cache = FALSE}
meth <- make.method(mammalsleep)
meth
```
and a `predictorMatrix`:
```{r}
pred <- make.predictorMatrix(mammalsleep)
pred
```

We add the call for passive imputation to the `ts` element in the `meth` object 
```{r}
meth["ts"]<- "~ I(sws + ps)"
meth
```

and set the predictor relations for `ts` with `sws` and `ps` to `0`. Also, we have to exclude `Species` as a predictor
```{r}
pred[c("sws", "ps"), "ts"] <- 0
pred[, "species"] <- 0
pred
```
This avoids circularity problems where `ts` would *feed back into* `sws` and `ps`, from which it is calculated:

We can then run the imputations as
```{r}
pas.imp <- mice(mammalsleep, 
                meth = meth, 
                pred = pred, 
                maxit = 50, 
                seed = 123, 
                print = F)
```

We used a custom predictor matrix and method vector to tailor our imputation approach to the passive imputation problem. We made sure to exclude `ts` as a predictor for the imputation of `sws` and `ps` to avoid circularity. 

We also gave the imputation algorithm 10 iterations to converge and fixed the seed to `123` for this `mice` instance. This means that even when people do not fix the overall `R` seed for a session, exact replication of results can be obtained by simply fixing the `seed` for the random number generator within `mice`. Naturally, the same input (data) is each time required to yield the same output (`mids`-object). 

When we study convergence, we see that the apparent non-convergence that we saw before has now disappeared with the use of passive imputation for the deterministic system `(ts, ps, sws)`. 

```{r}
plot_trace(pas.imp)
```


---

## For fun

**What you shouldn’t do with passive imputation!**

Never set all relations fixed. You will remain with the starting values and waste your computer’s energy (and your own).
```{r cache = FALSE}
meth <- make.method(boys)
pred <- make.predictorMatrix(boys)
meth["bmi"] <- "~ I(wgt / (hgt / 100)^2)"
meth["wgt"] <- "~ I(bmi * (hgt / 100)^2)"
meth["hgt"] <- "~ I(sqrt(wgt / bmi) * 100)"
pred[c("bmi", "wgt", "hgt"), ] <- 0
imp.path <- mice(boys, 
                 meth=meth, 
                 pred=pred, 
                 seed=123)
plot_trace(imp.path, c("hgt", "wgt", "bmi"))
```

We named the `mids` object `imp.path`, because the nonconvergence is pathological in this example!

---

# Conclusion
We have seen that the practical execution of multiple imputation and pooling is straightforward with the `R` package `mice`. The package is designed to allow you to assess and control the imputations themselves, the convergence of the algorithm and the distributions and multivariate relations of the observed and imputed data. 

It is important to 'gain' this control as a user. After all, we are imputing values and we aim to properly adress the uncertainty about the missingness problem. 

---

# Additional materials
A more detailed practical guide to `mice` in `R` can be found [here](https://www.gerkovink.com/miceVignettes/)

---

# References

Rubin, D. B. *Multiple imputation for nonresponse in surveys*. John Wiley & Sons, 1987. [Amazon](http://www.amazon.com/Multiple-Imputation-Nonresponse-Surveys-Donald/dp/0471655740/ref=sr_1_1?ie=UTF8&qid=1434466788&sr=8-1&keywords=Multiple+imputation+for+nonresponse+in+surveys)

Schafer, J.L. (1997). *Analysis of Incomplete Multivariate Data*. London: Chapman & Hall. Table 6.14. [Amazon](http://www.amazon.com/Incomplete-Multivariate-Monographs-Statistics-Probability/dp/0412040611/ref=sr_1_1?ie=UTF8&qid=1434466828&sr=8-1&keywords=Analysis+of+Incomplete+Multivariate+Data)

Van Buuren, S. and Groothuis-Oudshoorn, K. (2011). mice: Multivariate Imputation by Chained Equations in R. *Journal of Statistical Software*, 45(3), 1-67. [pdf](http://www.jstatsoft.org/v45/i03/paper)

---

**- End of practical**

---

```{r}
sessionInfo()
```

