# Statistische Analyse {#sec-mt4}

---
author: 
  - name: "Gerko Vink"
    orcid: "0000-0001-9767-1924"
    email: g.vink@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
format:
  html:
    highlight-style: github
    number-sections: false
    toc: true
    toc-depth: 3
    code-fold: true
---

::: callout-tip
## Hoorcollege 5 juni 2025
Vandaag gaan we de diepte in. We zullen leren hoe we uit data een model kunnen halen, en hoe we dat model kunnen gebruiken om statistische testen uit te voeren. Stiekem zullen alle modellen van vandaag in meer of mindere mate terugvoeren op *least squares* methodologie - de methoden die gebruik maken van de kleinste gekwadrateerde verschillen om een schatting te kunnen verkrijgen. De slides voor het hoorcollege kunt u [hier vinden](slides/lec-4.html)
:::

---

# Exercises

---

## Load packages
```{r}
#| code-fold: false
library(tidyverse) # for data manipulation and visualization
# tidyverse loads readr, dplyr, tidyr, tibble, stringr, etcetera
# all these packages we need in some form in this practical
library(magrittr)  # for flexible pipes
```

---

## Linear model

__Exercise 1:__ **Fit the following linear models on the `anscombe` data:**

- `y1` modeled by `x1` - stored (i.e. assign with `<-`) in object `fit1`
- `y2` modeled by `x2` - stored (i.e. assign with `<-`) in object `fit2`
- `y3` modeled by `x3` - stored (i.e. assign with `<-`) in object `fit3`
- `y4` modeled by `x4` - stored (i.e. assign with `<-`) in object `fit4`

```{r}
fit1 <- anscombe %$%
  lm(y1 ~ x1)
fit2 <- anscombe %$%
  lm(y2 ~ x2)
fit3 <- anscombe %$%
  lm(y3 ~ x3)
fit4 <- anscombe %$%
  lm(y4 ~ x4)
```

It is a convention to call an object that contains a fitted model (`lm(y1 ~ x1)` results in a fitted linear model) by its name prefixed with `fit`. This is not a requirement, but it is a convention that makes it easier to read the code. Anyone who sees `fit1` knows that this is a fitted model - and most likely the first one - in a series of fitted models.

---

## Structure the output

__Exercise 2:__ **Create a data frame from the coefficients of the 4 fitted objects from Exercise 1 and name the resulting object `fitted_coefs`**
We did not yet discuss this in the lecture, but the `coef()` function returns the coefficients of a fitted model. We can use this to quickly extract the coefficients of fitted models to create a data frame with the coefficients of the four fitted `anscombe` models. 

```{r}
fitted_coefs <- data.frame(fit1 = coef(fit1),
                           fit2 = coef(fit2),
                           fit3 = coef(fit3),
                           fit4 = coef(fit4))
fitted_coefs
```

These estimates are very similar, as we already established in today's lecture. For the interpretation of the estimates: 

	-	The intercept represents the expected value of the outcome ($y$) when all predictor variables ($x$) are zero.
	-	Each regression estimate (coefficient) shows the expected change in $y$ for a one-unit increase in that predictor, holding all other variables constant.
	-	A positive coefficient means $y$ increases as the predictor $x$ increases; a negative coefficient means $y$ decreases with increasing $x$.

---

__Exercise 3:__ **Now add the rownames as a column to the data and make sure that the resulting object is a tibble.**
```{r}
fitted_coefs %<>% # assign pipe parses the result back to fitted_coefs
  rownames_to_column(var = "estimate") %>% # add rownames to the data
  as_tibble() # explicitly make it a tibble
```

---

__Exercise 4:__ **Inspect the `fitted_coefs` data set. Why does `fit2` have fewer decimals printed than the other models?**

```{r}
fitted_coefs

#The coefficient for `fit2` is exactly `0.5`, which is printed with only one decimal, while the other coefficients are printed with 3 digits. Tibbles print numbers with three significant digits by default, switching to scientific notation if the available space is too small. If it can be represented with fewer numbers, it will do so. 
```

---

## Plotting the models

__Exercise 5:__ **Plot the four fitted models from the anscombe data set in a single plotting window as follows: use `plot()` to make a plot for the first model and `points()` to add each subsequent model's points to the plot window. Make the points for the models in the plots `blue` (`y1 ~ x1`), `gray` (`y2 ~ x2`), `orange` (`y3 ~ x3`) and `purple` (`y4 ~ x4`), respectively. **

Hint: look at `?plot` and `?points` for the arguments that you need to plot the models and to change the color of the points. 

```{r}
plot(y1 ~ x1, col = "blue", data = anscombe)
points(y2 ~ x2, col = "gray", data = anscombe)
points(y3 ~ x3, col = "orange", data = anscombe)
points(y4 ~ x4, col = "purple", data = anscombe)
```

---

__Exercise 6:__ **Now plot all four fitted models in a plotting window with 2 rows and 2 columns.**

Hint: use the command `par(mfrow = c(2, 2))` to set the plotting window to 2 rows and 2 columns. Use this command before you start the plots. You do not have to use `points()` in this case, but rather use `plot()` for every seperate model plot. 

```{r}
par(mfrow = c(2, 2))
plot(y1 ~ x1, data = anscombe)
plot(y2 ~ x2, data = anscombe)
plot(y3 ~ x3, data = anscombe)
plot(y4 ~ x4, data = anscombe)
```
With `par(mfrow = c(2, 2))` we set the plotting window to have 2 rows and 2 columns. This means that the first plot will be placed in the first row, first column, the next plot will be placed in the first row, second column, and so on.

To revert to a single plotting window, you can use `par(mfrow = c(1, 1))` after the plots.

```{r}
par(mfrow = c(1, 1)) # revert to single plotting window 
```


---

## Read in the `titanic` data set

We use the `titanic` data set for the next exercises. Use the code below to read them in from the internet:

```{r}
con <- url("https://www.gerkovink.com/erasmus/Day%202/Part%20D/titanic.csv")
titanic <- read_csv(con) # reads in the connection as a csv file
```

---

__Exercise 7:__ **Inspect the titanic data set by calling `titanic` in the console and with functions `summary()`, `str()` and `glimpse`.**

- `titanic` will print the data set in the console:
```{r}
titanic
```
We can see that the `titanic` data set is imported as a `tibble`. A `tibble` is a more flexible data frame with a much nicer printing class.

- `summary()` gives us a quick overview of the data set:
```{r}
titanic %>% summary()
```
The `summary()` output gives us direct information about the parametric nature of the columns in the data

- `str()` gives us the structure of the data set:
```{r}
titanic %>% str()
```
When we study the structure of the data set, we see that the outcome `Survived` is not coded as a `factor`, but as a numeric column. The same holds for `Pclass`. This will influence the default estimation later on. There are more irregularities, but we'll ignore those for now.
```{r}
titanic %>% glimpse()
```
There are no missing values in this `titanic` data set. 

---

__Exercise 8:__ **Correct the measurement level of the columns `Pclass` and `Survived`. Then ask for the `summary()` once more.**
```{r}
titanic %<>% 
  mutate(Pclass = factor(Pclass, labels = c("1st class", "2nd class", "3rd class")), 
         Survived = factor(Survived, labels = c("No", "Yes")))
titanic %>% summary()
```
We now see the tabular information about the `Survived` and `Pclass` columns. This is because these columns are now coded as factors (i.e. categorical variables with a numeric representation). Note that in the `mutate` call, I used the ` %<>% ` pipe. This *assign* pipe returns the endresult of the pipe to the original object. This mitigates the use of the `<-` assign operator and the double calling of the `titanic` set in the regular strategy below:

```{r eval = FALSE}
titanic <- titanic %>% 
  mutate(Pclass = factor(Pclass, labels = c("1st class", "2nd class", "3rd class")), 
         Survived = factor(Survived, labels = c("No", "Yes")))
```

---

## Correlations

__Exercise 9:__ **Calculate Pearson's correlation coefficient between `Age` and `Fare` in the `titanic` data set.**

```{r}
titanic %>% 
  summarise(correlation = cor(Age, Fare, use = "pairwise.complete.obs"))
# or
titanic %$%
  cor(Age, Fare, use = "pairwise.complete.obs")
```

---

## T-test


__Exercise 10:__ **Calculate the mean and variance of age for the survivors and the non-surivors using `summarise()`**
```{r}
titanic %>%
  group_by(Survived) %>%
  summarise(mean_age = mean(Age, na.rm = TRUE),
            var_age = var(Age, na.rm = TRUE))
```

---

__Exercise 11:__ **Perform a t-test to compare the mean `Age` of passengers that survived and those that did not. Perform the t-test once with equal variances assumed, and once without. Does the conclusion change?**
```{r}
titanic %$%
  t.test(Age ~ Survived, var.equal = TRUE)
titanic %$%
  t.test(Age ~ Survived, var.equal = FALSE)
```

There does not seem to be a significant difference in age between the survivors and the non-survivors. The p-value is larger than 0.05 in both cases, so we cannot reject the null hypothesis that the means are equal.

---

## Chi-squared test

__Exercise 12:__ **Perform a chi-squared test to compare the survival rates of passengers in the different classes. Also study the observed frequencies table and study the expected frequencies.**

```{r}
titanic %$%
  table(Pclass, Survived) # Create a contingency table
titanic %$%
  chisq.test(table(Pclass, Survived)) # Perform the chi-squared test
titanic %$%
  chisq.test(table(Pclass, Survived)) %>% 
  .$expected # Study the expected frequencies
```

The observed frequencies show that there are more survivors in the 1st class than in the 2nd and 3rd class. 

The $\chi^2$-test indicates a significant difference in survival rates between the different classes, as the p-value is smaller than 0.05. The expected frequencies are all larger than 5, so we can safely use the chi-squared test. 

Alternatively, we could have used `broom::tidy()` to tidy the output of the chi-squared test:
```{r}
library(broom) # for tidy output
titanic %$%
  chisq.test(table(Pclass, Survived)) %>% 
  broom::tidy()
```

---

## ANOVA

__Exercise 13:__ **Calculate the mean and variance of `Fare` for every passenger class using `summarise()`**
```{r}
titanic %>% 
  group_by(Pclass) %>% 
  summarise(mean_fare = mean(Fare, na.rm = TRUE),
            var_fare = var(Fare, na.rm = TRUE))
```

There is a visible difference in mean fare between the different classes, as the variance is larger for the *more luxurious* classes. The mean fare for 1st class is much higher than for 2nd and 3rd class, which is expected.

---

__Exercise 14:__ **Perform an ANOVA to compare the mean `Fare` of passengers in the different classes.**

```{r}
titanic %$%
  lm(Fare ~ Pclass) %>%
  anova()
# or
titanic %$%
  aov(Fare ~ Pclass) %>%
  summary()
```
The ANOVA shows that there is a significant difference in mean fare between the different classes, as the p-value is smaller than 0.05.

---

End of `Practical`. 
