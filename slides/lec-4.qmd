---
title: "Statistische Analyse"
author: 
  - name: Gerko Vink
    orcid: 0000-0001-9767-1924
    email: g.vink@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
date: 2 June 2025
date-format: "D MMM YYYY"
execute: 
  echo: true
format: 
  revealjs:
    theme: [solarized, gerko.scss]
    embed-resources: true
    progress: true
    margin: 0.075
    logo: logo.png 
    toc: false
    toc-depth: 1
    toc-title: Outline
    slide-number: true
    scrollable: false
    width: 1200
    reference-location: margin
    footer: Gerko Vink @ Anton de Kom Universiteit, Paramaribo
    standalone: true
---

## Disclaimer {.smaller}
I owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.

Images are either directly linked, or generated with StableDiffusion or DALL-E. That said, there is no information in this presentation that exceeds legal use of copyright materials in academic settings, or that should not be part of the public domain. 

::: {.callout-warning}
You **may use** any and all content in this presentation - including my name - and submit it as input to generative AI tools, with the following **exception**:

- You must ensure that the content is not used for further training of the model
:::

## Slide materials and source code
::: callout-tip
# Materials
- lecture slides on Moodle
- course page: [www.gerkovink.com/sur](https://www.gerkovink.com/sur)
- source: [github.com/gerkovink/sur](https://github.com/gerkovink/sur)
:::

## Recap

Gisteren hebben we deze onderwerpen behandeld:

- Het combineren van datasets
- Groeperen en aggregeren
- Nieuwe variabelen creëren
- Filteren en sorteren van gegevens
- Het maken en aanpassen van datagroepen
- Clustering van gegevens


## Today
Vandaag behandelen we de volgende onderwerpen:

- Beschrijvende statistiek
- Kruistabellen en frequentieverdelingen
- X2
- toets- en associatiematen
- Simpele lineaire regressie
- Analyses draaien op groepen

##  We use the following packages
```{r message=FALSE}
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
library(DAAG)
library(car)

set.seed(123)
```

# Linear regression


## Linear Model

The linear regression model:

$$y_i=\beta_0+\sum_{j}\beta_{j} x_{ij}+\varepsilon_i, \ \ \ \ \ \  \varepsilon_i\sim N(0, \sigma^2)$$
where

- $y_i$ is score of individual $i$ on the numeric dependent variable $Y$

- $x_{ij}$ is the score of individual $i$ on predictor $X_j$  

- $\beta_0$ is the intercept 

- $\beta_j$ is the slope of $X_j$

- $\varepsilon_{i}$ is the residual (prediction error)


## The `lm()` function

```{r eval = FALSE}
lm(formula, data) # returns an object of class lm
```

formula        | model
:--------------|--------------------------
`y ~ 1`        | intercept-only
`y ~ x1`       | main effect of x1
`y ~ x1 + x2`  | main effects of x1, x2
`y ~ .`        | main effects of all predictors
`y ~ . - x1`   | main effects of all predictors except x1
`y ~ x1 + x2 + x1:x2`    | main effects + interaction between x1 and x2
`y ~ x1*x2`    | idem
`y ~ .^2`      | main effects + pairwise interactions between all predictors


# Simple regression 

## Continuous predictor


- observed = fitted + residual

$$y_i=\hat{y}_i+\varepsilon_i$$
- fitted = intercept + slope times x

$$\hat{y}_i=\beta_0 + \beta_1x_i$$

<br>

- fitted changes with $\beta_1$ for each unit increase in $x$

## Regression line for `mpg ~ disp`

```{r message = FALSE}
ggplot(mtcars, aes(disp, mpg)) + 
  geom_point() +
  xlim(0, 500) +
  geom_smooth(method = "lm", se = FALSE, fullrange=T)
```

## Coefficients

Interpretation of the parameter estimates.

```{r}
fit <- lm(mpg ~ disp, mtcars)
```

## Structure of `lm` object

```{r}
str(fit)
```

## Summary of `lm` object 

```{r}
summary(fit)
```

## Structure summary `lm` object

```{r}
str(summary(fit))
```


## Extracting  `lm` list elements:


Function / Subsetting        | Output
-----------------------------|-------------------------
`coef(fit) / fit$coef`       | coefficients
`fitted(fit) / fit$fitted`   | fitted values
`resid(fit) / fit$resid`     | residuals
`summary(fit)$r.squared`     | R-squared statistic

```{r collapse = T}
fit$coef
summary(fit)$r.squared
```



# Categorical predictor 


## Dummy variables

Categorical predictors are converted into dummy variables: 

- each category has a dummy with value 1 for that category, and 0 otherwise

- except for the reference category  (0 on all dummies)

- all categories are compared to the reference category



```{r echo = FALSE, out.width = "70%", fig.cap = "Reference category of $z$ is $a$", fig.align = "center"}
knitr::include_graphics("img/lec-3/dummies.png")
```


## Interpreting dummies

Model for categorical $Z$ with categories $a, b, c$:

$$\hat{y}=\beta_0+\beta_1zb+\beta_2zc$$

<br>

parameters        | interpretation
------------------|---------------
$\beta_0$         | predicted mean category $a$ (reference category)
$\beta_0+\beta_1$ | predicted mean category $b$
$\beta_0+\beta_2$ | predicted mean category $c$



## Example

Interpret the parameter estimates of model `mpg ~ factor(am)` 

- `am = 0` is  automatic and `am = 1` is manual transmission

- reference category is `am = 0`


```{r}
coef(lm(mpg ~ factor(am), mtcars))
```
# Linear regression


## Linear Model

The linear regression model:

$$y_i=\beta_0+\sum_{j}\beta_{j} x_{ij}+\varepsilon_i, \ \ \ \ \ \  \varepsilon_i\sim N(0, \sigma^2)$$
where

- $y_i$ is score of individual $i$ on the numeric dependent variable $Y$

- $x_{ij}$ is the score of individual $i$ on predictor $X_j$  

- $\beta_0$ is the intercept 

- $\beta_j$ is the slope of $X_j$

- $\varepsilon_{i}$ is the residual (prediction error)


## The `lm()` function

```{r eval = FALSE}
lm(formula, data) # returns an object of class lm
```

formula        | model
:--------------|--------------------------
`y ~ 1`        | intercept-only
`y ~ x1`       | main effect of x1
`y ~ x1 + x2`  | main effects of x1, x2
`y ~ .`        | main effects of all predictors
`y ~ . - x1`   | main effects of all predictors except x1
`y ~ x1 + x2 + x1:x2`    | main effects + interaction between x1 and x2
`y ~ x1*x2`    | idem
`y ~ .^2`      | main effects + pairwise interactions between all predictors


# Simple regression 

## Continuous predictor


- observed = fitted + residual

$$y_i=\hat{y}_i+\varepsilon_i$$
- fitted = intercept + slope times x

$$\hat{y}_i=\beta_0 + \beta_1x_i$$

<br>

- fitted changes with $\beta_1$ for each unit increase in $x$

## Regression line for `mpg ~ disp`

```{r message = FALSE}
ggplot(mtcars, aes(disp, mpg)) + 
  geom_point() +
  xlim(0, 500) +
  geom_smooth(method = "lm", se = FALSE, fullrange=T)
```

## Coefficients

Interpretation of the parameter estimates.

```{r}
fit <- lm(mpg ~ disp, mtcars)
```

## Structure of `lm` object

```{r}
str(fit)
```

## Summary of `lm` object 

```{r}
summary(fit)
```

## Structure summary `lm` object

```{r}
str(summary(fit))
```


## Extracting  `lm` list elements:


Function / Subsetting        | Output
-----------------------------|-------------------------
`coef(fit) / fit$coef`       | coefficients
`fitted(fit) / fit$fitted`   | fitted values
`resid(fit) / fit$resid`     | residuals
`summary(fit)$r.squared`     | R-squared statistic

```{r collapse = T}
fit$coef
summary(fit)$r.squared
```



# Categorical predictor 


## Dummy variables

Categorical predictors are converted into dummy variables: 

- each category has a dummy with value 1 for that category, and 0 otherwise

- except for the reference category  (0 on all dummies)

- all categories are compared to the reference category



```{r echo = FALSE, out.width = "70%", fig.cap = "Reference category of $z$ is $a$", fig.align = "center"}
knitr::include_graphics("img/lec-3/dummies.png")
```


## Interpreting dummies

Model for categorical $Z$ with categories $a, b, c$:

$$\hat{y}=\beta_0+\beta_1zb+\beta_2zc$$

<br>

parameters        | interpretation
------------------|---------------
$\beta_0$         | predicted mean category $a$ (reference category)
$\beta_0+\beta_1$ | predicted mean category $b$
$\beta_0+\beta_2$ | predicted mean category $c$



## Example

Interpret the parameter estimates of model `mpg ~ factor(am)` 

- `am = 0` is  automatic and `am = 1` is manual transmission

- reference category is `am = 0`


```{r}
coef(lm(mpg ~ factor(am), mtcars))
```

# Model fit

## A simple model
```{r cache = FALSE}
boys.fit <- 
  na.omit(boys) %$% # Extremely wasteful
  lm(age ~ reg)
boys.fit

boys %>% na.omit(boys) %$% aggregate(age, list(reg), mean)
```

## Plotting the model
```{r fig.height=3.5}
means <- boys %>% na.omit(boys) %>% group_by(reg) %>% summarise(age = mean(age))
ggplot(na.omit(boys), aes(x = reg, y = age)) + 
  geom_point(color = "grey") + 
  geom_point(data = means, stat = "identity", size = 3)
```


## Model parameters
```{r cache = FALSE}
boys.fit %>%
  summary()
```

## Is it a good model? {.smaller}
```{r cache = FALSE}
boys.fit %>%
  anova()
```

It is not a very informative model. The `anova` is not significant, indicating that the contribution of the residuals is larger than the contribution of the model. 

The outcome `age` does not change significantly when `reg` is varied. 

## Model factors
```{r cache = FALSE}
boys.fit %>%
  model.matrix() %>%
  head(n = 10)
```

`R` expands the categorical variable for us

  - it dummy-codes the `5` categories into `4` dummies (and an intercept). 

## Post hoc comparisons
```{r cache = FALSE}
coef <- boys.fit %>% aov() %>% summary.lm()
coef
```

## Post hoc comparisons
Without adjustments for the p-value
```{r cache = FALSE}
na.omit(boys) %$% pairwise.t.test(age, reg, p.adj = "none")
```

## Post hoc comparisons
With adjusted p-values cf. Bonferoni correction
```{r cache = FALSE}
na.omit(boys) %$% pairwise.t.test(age, reg, p.adj = "bonf")
```

## Post hoc comparisons
Manually calculated
```{r}
p.val <- coef$coefficients
p.adjust(p.val[, "Pr(>|t|)"], method = "bonferroni")
```

If you have trouble reading scientific notation, `5.077098e-68` means the following

$$5.077098\text{e-68} = 5.077098 \times 10^{-68} = 5.077098 \times (\frac{1}{10})^{-68}$$

This indicates that the comma should be moved 68 places to the left:

$$5.077098\text{e-68} = .000000000000000000000000000000000000$$
$$000000000000000000000000000000005077098$$

## AIC
Akaike's *An Information Criterion* 
```{r cache = FALSE}
boys.fit %>% 
  AIC()
```

### What is AIC
AIC comes from information theory and can be used for model selection. The AIC quantifies the information that is lost by the statistical model, through the assumption that the data come from the same model. In other words: AIC measures the fit of the model to the data. 

- The better the fit, the less the loss in information
- AIC works on the log scale:
  - $\text{log}(0) = -\infty$, $\text{log}(1) = 0$, etc. 
- the closer the AIC is to $-\infty$, the better


# Model comparison

## A new model
Let's add predictor `hgt` to the model:
```{r cache = FALSE}
boys.fit2 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt)

boys.fit %>% AIC()
boys.fit2 %>% AIC()
```

## Another model
Let's add `wgt` to the model
```{r cache = FALSE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt)
```

## And another model
Let's add `wgt` and the interaction between `wgt` and `hgt` to the model
```{r cache = FALSE}
boys.fit4 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt)
```
is equivalent to 
```{r eval=FALSE}
boys.fit4 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt + hgt:wgt)
```

## Model comparison
```{r cache = FALSE}
boys.fit %>% AIC()
boys.fit2 %>% AIC()
boys.fit3 %>% AIC()
boys.fit4 %>% AIC()
```

## Another form of model comparison
```{r cache = FALSE}
anova(boys.fit, boys.fit2, boys.fit3, boys.fit4)
```

## Inspect `boys.fit3`
```{r cache = FALSE}
boys.fit3 %>% anova()
```

## Inspect `boys.fit4`
```{r cache = FALSE}
boys.fit4 %>% anova()
```
It seems that `reg` and the interaction `hgt:wgt` are redundant

## Remove `reg`
```{r}
boys.fit5 <- 
  na.omit(boys) %$%
  lm(age ~ hgt + wgt)
```
Let's revisit the comparison
```{r}
anova(boys.fit, boys.fit2, boys.fit3, boys.fit5)
```
But the `boys.fit5` model is better than the previous model with fewer parameters

## Stepwise regression
We start with the full model, which contains all parameters for all columns. 

The most straightforward way to go about this is by specifying the following model:
```{r}
full.model <- lm(age ~ ., data = na.omit(boys))
full.model
```

## Stepwise regression - continued
We can then start with specifying the stepwise model. In this case we choose direction `both`. 
```{r}
step.model <- step(full.model, direction = "both", 
                      trace = FALSE)
step.model
```

Other options are 

- `forward`: fit all univariate models, add the best predictor and continue.
- `backward`: fit the full model, eliminate the worst predictor and continue. 

## Summary
```{r}
step.model %>% summary
```

## Stepwise regression - AIC
```{r}
full.model <- lm(age ~ ., data = na.omit(boys))
step.model <- MASS::stepAIC(full.model, direction = "both", 
                      trace = FALSE)
step.model
```

## Influence of cases
DfBeta calculates the change in coefficients depicted as deviation in SE's.
```{r cache = FALSE}
step.model %>%
  dfbeta() %>%
  head(n = 7)
```

# Prediction

## Fitted values
Let's use the simpler `anscombe` data example
```{r cache = FALSE}
fit <- anscombe %$% lm(y1 ~ x1)

y_hat <- 
  fit %>%
  fitted.values()
```
The residual is then calculated as
```{r cache = FALSE}
y_hat - anscombe$y1
```

## Predict new values
If we introduce new values for the predictor `x1`, we can generate predicted values from the model
```{r cache = FALSE, warning=FALSE}
new.x1 <- data.frame(x1 = 1:20)
fit %>% predict(newdata = new.x1)
```

## Predictions are draws from the regression line
```{r}
pred <- fit %>% predict(newdata = new.x1)
lm(pred ~ new.x1$x1)$coefficients
fit$coefficients
```

## Prediction intervals
```{r warning=FALSE}
fit %>% predict(interval = "prediction")
```

A prediction interval reflects the uncertainty around a single value. The confidence interval 
reflects the uncertainty around the mean prediction values. 

## How many cases are used?
```{r}
na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

If we would not have used `na.omit()`
```{r}
boys %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

# Confidence intervals?

## 95\% confidence interval {.smaller}
*If an infinite number of samples were drawn and CI's computed, then the true population mean $\mu$ would be in* ***at least*** *95\% of these intervals*

\[
95\%~CI=\bar{x}\pm{t}_{(1-\alpha/2)}\cdot SEM
\]

Example
```{r}
x.bar <- 7.6 # sample mean
SEM   <- 2.1 # standard error of the mean
n     <- 11 # sample size
df    <- n-1 # degrees of freedom
alpha <- .15 # significance level
t.crit <- qt(1 - alpha / 2, df) # t(1 - alpha / 2) for df = 10
c(x.bar - t.crit * SEM, x.bar + t.crit * SEM) 
```

## {.smaller}
<center>
<img src="img/lec-4/Neyman1934.png" alt="HTML5 Icon" width = 75%>
</center>

        Neyman, J. (1934). On the Two Different Aspects of the Representative Method: 
        The Method of Stratified Sampling and the Method of Purposive Selection. 
        Journal of the Royal Statistical Society, Vol. 97, No. 4 (1934), pp. 558-625

## Misconceptions {.smaller}
Confidence intervals are frequently misunderstood, even well-established researchers sometimes misinterpret them. .

1. A realised 95% CI does not mean:

- that there is a 95% probability the population parameter lies within the interval
- that there is a 95% probability that the interval covers the population parameter

    Once an experiment is done and an interval is calculated, the interval either covers, or does       not cover the parameter value. Probability is no longer involved. 

    The 95% probability only has to do with the estimation procedure. 

2. A 95% confidence interval does not mean that 95% of the sample data lie within the interval.
3. A confidence interval is not a range of plausible values for the sample mean, though it may be understood as an estimate of plausible values for the population parameter.
4. A particular confidence interval of 95% calculated from an experiment does not mean that there is a 95% probability of a sample mean from a repeat of the experiment falling within this interval.

## Confidence intervals
```{r fig.height = 4, echo=FALSE, message=FALSE, message=FALSE, warning = FALSE}
set.seed(1234)
library(plyr)
samples <- rlply(100, rnorm(5000, mean = 0, sd = 1))
info <- function(x){ 
  M <- mean(x)
  DF <- length(x) - 1
  SE <- 1 / sqrt(length(x))
  INT <- qt(.975, DF) * SE
  return(c(M, M - 0, SE, M - INT, M + INT))
}
format <- c("Mean" = 0, "Bias" = 0, "Std.Err" = 0, "Lower" = 0, "Upper" = 0)
require("magrittr")
results <- samples %>%
  vapply(., info, format) %>%
  t()
results <- results %>%
  as.data.frame() %>%
  mutate(Covered = Lower < 0 & 0 < Upper)
require(ggplot2)
limits <- aes(ymax = results$Upper, ymin = results$Lower)
ggplot(results, aes(y=Mean, x=1:100, colour = Covered)) + 
  geom_hline(aes(yintercept = 0), color = "dark grey", size = 2) + 
  geom_pointrange(limits) + 
  xlab("Simulations 1-100") +
  ylab("Means and 95% Confidence Intervals")
```

100 simulated samples from a population with $\mu = 0$ and $\sigma^2=1$. Out of 100 samples, only 5 samples have confidence intervals that do not cover the population mean.

## So far

At this point we have covered the following models:

- Simple linear regression (SLR)

\[y=\alpha+\beta x+\epsilon\]

*The relationship between a numerical outcome and a numerical or categorical predictor*

- Multiple linear regression (MLR)

\[y=\alpha+\beta_1 x_1 + \beta_2 x_2 + \dots \beta_p x_p + \epsilon\]

*The relationship between a numerical outcome and **multiple** numerical or categorical predictors*

### What remains
We have not yet covered how to handle outcomes that are not categorical or how to deal with predictors that are nonlinear or have a strict dependency structure. 

## What we have learned
We have covered the following topics last week:

- fit SLR and MLR models
- select MLR models
- interpret model parameters
- perform hypothesis test on slope and intercept parameters
- perform hypothesis test for the whole regression model
- calculate confidence intervals for regression parameters
- obtain prediction intervals for fitted values
- study the influence of single cases
- study the validity of linear regression assumptions:
  - linearity, constant residual variance
- study the residuals, leverage and Cook's distance

## Rewriting what we know
Instead of modeling

\[y=\alpha+\beta x+\epsilon\]

we can also consider 
\[\mathbb{E}[y] = \alpha + \beta x\]

They're the same. Different notation, different framework.

The upside is that we can now use a function for the expectation $\mathbb{E}$ to allow for transformations. This would enable us to change $\mathbb{E}[y]$ such that $f(\mathbb{E}[y])$ has a linear relation with $x$.

This is what we will be doing today

# Illustration of the problem

## A `simulated` data set
To further illustrate why the linear model is not an appropriate model for discrete data I propose the following simple simulated data set:
```{r fig.height=3}
set.seed(123)
simulated <- data.frame(discrete = c(rep(0, 50), rep(1, 50)),
                        continuous = c(rnorm(50, 10, 3), rnorm(50, 15, 3)))

simulated %>% summary
```
This data allows us to illustrate modeling the relation between the `discrete` outcome and the `continuous` predictor with logistic regression. 

Remember that fixing the random seed allows for a replicable random number generator sequence. 

## Visualizing `simulated` data
```{r fig.height=3}
simulated %>% ggplot(aes(x = continuous, y = discrete)) +
  geom_point()
```

## Modeling `simulated` with `lm`
```{r fig.height=3, message=FALSE}
simulated %>% ggplot(aes(x = continuous, y = discrete)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE, color = "orange") 
```

The orange line represents the `lm` linear regression line. It is not a good representation for our data, as it assumes the data are continuous and projects values outside of the range of the observed data. 

## Modeling `simulated` with `glm`
```{r fig.height=3, message=FALSE}
simulated %>% ggplot(aes(x = continuous, y = discrete)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE, color = "orange") +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) 
```

The blue `glm` logistic regression line represents this data infinitely better than the orange `lm` line. It assumes the data to be `0` or `1` and does not project values outside of the range of the observed data. 

# How does this work?

## Generalized linear modeling
There is a very general way of addressing this type of problem in regression. The models that use this *general way* are called generalized linear models (GLMs). 
 
Every generalized linear model has the following three characteristics:

1. A probability distribution that describes the outcome
2. A linear predictor model
3. A link function that relates the linear predictor to the the parameter of the outcome's probability distribution. 

The linear predictor model in (2) is 
$$\eta = \bf{X}\beta$$
where $\eta$ denotes a linear predictor and the link function in (3) is 
$$\bf{X}\beta = g(\mu)$$
The technique to model a binary outcome based on a set of continuous or discrete predictors is called *logistic regression*. **Logistic regression is an example of a generalized linear model.** 
 
## The link function
The link function for logistic regression is the `logit link`

$$\bf{X}\beta = ln(\frac{\mu}{1 - \mu})$$ 

where $$\mu = \frac{\text{exp}(\bf{X}\beta)}{1 + \text{exp}(\bf{X}\beta)} = \frac{1}{1 + \text{exp}(-\bf{X}\beta)}$$

Before we continue with discussing the link function, we are first going to dive into the concept of odds. 

Properly understanding odds is necessary to perform and interpret logistic regression, as the `logit` link is connected to the odds

## Modeling the odds
Odds are a way of quantifying the probability of an event $E$

The odds for an event $E$ are 
$$\text{odds}(E) = \frac{P(E)}{P(E^c)} = \frac{P(E)}{1 - P(E)}$$
The odds of getting heads in a coin toss is

$$\text{odds}(\text{heads}) = \frac{P(\text{heads})}{P(\text{tails})} = \frac{P(\text{heads})}{1 - P(\text{heads})}$$
For a fair coin, this would result in 

$$\text{odds}(\text{heads}) = \frac{.5}{1 - .5} = 1$$

## Another odds example

The game [Lingo](lingo link) has 44 balls: 36 blue, 6 red and 2 green balls

- The odds of a player choosing a blue ball are $$\text{odds}(\text{blue}) = \frac{36}{8} =  \frac{36/44}{8/44} = \frac{.8182}{.1818} = 4.5$$
- The odds of a player choosing a red ball are $$\text{odds}(\text{red}) = \frac{6}{38} = \frac{6/44}{36/44} = \frac{.1364}{.8636}\approx .16$$
- The odds of a player choosing a green ball are $$\text{odds}(\text{green}) = \frac{2}{42} = \frac{2/44}{42/44} = \frac{.0455}{.9545}\approx .05$$

Odds of 1 indicate an equal likelihood of the event occuring or not occuring. Odds `< 1` indicate a lower likelihood of the event occuring vs. not occuring. Odds `> 1` indicate a higher likelihood of the event occuring. 

## GLM's continued

Remember that
\[y=\alpha+\beta x+\epsilon,\]

and that 
\[\mathbb{E}[y] = \alpha + \beta x.\]

As a result
\[y = \mathbb{E}[y] + \epsilon.\]

and residuals do not need to be normal (heck, $y$ probably isn't, so why should $\epsilon$ be?)

## Logistic regression
Logistic regression is a GLM used to model a **binary categorical variable** using **numerical** and **categorical** predictors.

In logistic regression we assume that the true data generating model for the outcome variable follows a binomial distribution. 

  - it is therefore intuitive to think of logistic regression as modeling the probability of succes $p$ for any given set of predictors. 
  
### How
We specify a reasonable link that connects $\eta$ to $p$. Most common in logistic regression is the *logit* link

$$logit(p)=\text{log}(\frac{p}{1−p}) , \text{ for } 0 \leq p \leq 1$$
We might recognize $\frac{p}{1−p}$ as the odds.

## $\log(\text{odds})$ explained
Now if we visualize the relation between our predictor(s) and the logodds
```{r  dev.args = list(bg = 'transparent'), echo=FALSE}
fit <- simulated %$%
  glm(discrete ~ continuous, family = binomial())
linpred <- predict(fit, type = "link")
plot(simulated$continuous, linpred, xlab = "predictor space (fitted values)", ylab = "log(odds)")
```

## The link to the responses explained
And the relation between our predictor(s) and the probability
```{r  dev.args = list(bg = 'transparent'), echo = FALSE}
response <- predict(fit, type = "response")
plot(simulated$continuous, response, xlab = "predictor space (fitted values)", ylab = "probability")
```

# Logistic regression

## Logistic regression
With linear regression we had the `Sum of Squares (SS)`. Its logistic counterpart is the `Deviance (D)`. 

 -  Deviance is the fit of the observed values to the expected values. 
 
With logistic regression we aim to maximize the `likelihood`, which is equivalent to minimizing the deviance. 

The likelihood is the (joint) probability of the observed values, given the current model parameters.

In normally distributed data: $\text{SS}=\text{D}$.

## The logistic regression model
Remember the three characteristics for every generalized linear model:

1. A probability distribution that describes the outcome
2. A linear predictor model
3. A link function that relates the linear predictor to the the parameter of the outcome's probability distribution. 

For the logistic model this gives us:

1. $y_i \sim \text{Binom}(p_i)$
2. $\eta = \beta_0 + \beta_1x_1 + \dots + \beta_nx_n$
3. $\text{logit}(p) = \eta$

Simple substitution brings us at

$$p_i = \frac{\text{exp}(\eta)}{1+\text{exp}(\eta)} = \frac{\text{exp}(\beta_0 + \beta_1x_{1,i} + \dots + \beta_nx_{n,i})}{1+\text{exp}(\beta_0 + \beta_1x_{1,i} + \dots + \beta_nx_{n,i})}$$

# Fitting a logistic regression

## The `anesthetic` data
```{r, message = FALSE}
anesthetic %>% head(n = 10)
```

Thirty patients were given an anesthetic agent maintained at a predetermined level (`conc`) for 15 minutes before making an incision. It was then noted whether the patient moved, i.e. jerked or twisted.

## Fitting a logistic regression model

Fitting a `glm` in `R` is not much different from fitting a `lm`. We do, however, need to specify what type of `glm` to use by specifying both the `family` and the type of `link` function we need. 

For logistic regression we need the **binomial** family as the binomial distribution is the probability distribution that describes our outcome. We also use the `logit` link, which is the default for the binomial `glm` family. 


```{r,  dev.args = list(bg = 'transparent')}
fit <- anesthetic %$% 
  glm(nomove ~ conc, family = binomial(link="logit"))
fit
```

## The model parameters
```{r}
fit %>% summary
```

## The regression parameters
```{r}
fit %>% summary %>% .$coefficients
```

With every unit increase in concentration `conc`, the log odds of **not moving** increases with `r coef(fit)[2]`. This increase can be considered different from zero as the p-value is `r summary(fit)$coefficients[2, 4]`. 

In other words; an increase in `conc` will lower the probability of moving. We can verify this by modeling `move` instead of `nomove`:

```{r,  dev.args = list(bg = 'transparent')}
anesthetic %$% 
  glm(move ~ conc, family = binomial(link="logit")) %>%
  summary %>% .$coefficients
```

# However..

## Error

```{r}
library(caret)
pred <- fit %>% predict(type = "response")
confusionMatrix(data = factor(as.numeric(pred > 0.5), labels = c("move", "nomove")),
                reference = factor(anesthetic$nomove, labels = c("move", "nomove")))
```
## Issue

With the error of the model (or lack thereof) - comes a problem. 

1. Is the (lack of) error due to the modeling?
2. Would another model give us a different error?
3. Is the (lack of error) due to the data?
4. Would other data give us a different error?
<br>
If (1) and (2) are the case --> model selection needed to improve the model
<br>
But what if the `better model` is only due to our data?
<br>
If (3) and (4) are the case --> we need other data to validate that our model is reliable

## Some concepts

**Training** <br>
If the model will only fit well on the data is has been trained on, then we are **overfitting**. We have then successfully modeled not only the data, but also (much of) the noise.

- overfitted models have little bias, but high variance
- the opposite, **underfitting** occurs when the model is too simple and cannot capture the structure of the data. 
  - underfitted models have high bias, but low variance.

A great example is the [library of babel](https://libraryofbabel.info/search.cgi). It contains every phrase, page, etc. that will ever be written in English. However, it is a most inefficient way of writing beautiful literature. 

**Testing** <br>
To avoid overfitting we can train the model on one data set and test its performance on another (seperate) data set that comes from the same true data generating model. 

**Validation** <br>
If we are also optimizing hyperparameters, then the in-between-step of validation makes sense. You then train the initial model on one data set, validate its optimization on another data set and finally test its performance on the last data set. 

## On real data
Collecting multiple independent data sets to realize a true `train/validate/test` approach is a costly endeavor that takes up a lot of time and resources. 
<br><br>
**Alternative: splitting the observed data**<br>
We can randomly split the data into 2 parts: one part for training and one part for testing

- The downside to this approach is that everything becomes split-depended
  - in theory we could still obtain a good or bad performance because of the split. 
  - influential values that are not part of the training data will skew the test performance
  - variance is usually low, but bias can be higher

**Solution: K-fold crossvalidation**<br>
Partition the data into $K$ folds and use each fold once as the test set and the remaining $K-1$ folds as the training set. 

 - You'll have $K$ estimates of the test accuracy
 - These $K$ estimates balance bias and variance
 - A special case is when you take $K = N$. This is called *leave on out crossvalidation*
 
## Example
```{r}
set.seed(123)
library(caret)

# define training control
train_control <- trainControl(method = "cv", number = 3, savePredictions = TRUE)

# train the model on training set
model <- train(as.factor(move) ~ conc,
               data = DAAG::anesthetic,
               trControl = train_control,
               method = "glm",
               family = binomial(link = "logit"))

# print cv scores
model$results
```

## The predictions
```{r eval = FALSE}
model$pred
```
```{r echo = FALSE}
library(DT)
datatable(model$pred)
```

## For fun
<center> 
![](img/lec-4/going_out_for_a_byte.png){width=40%}
<br>[source](https://www.reddit.com/r/ProgrammerHumor/comments/8za9b6/hello_i_am_your_server_for_today/)
</center>
