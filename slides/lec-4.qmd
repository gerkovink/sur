---
title: "Statistical Analysis"
author: 
  - name: Gerko Vink
    orcid: 0000-0001-9767-1924
    email: g.vink@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
date: 5 June 2025
date-format: "D MMM YYYY"
execute: 
  echo: true
format: 
  revealjs:
    theme: [solarized, gerko.scss]
    embed-resources: true
    progress: true
    margin: 0.075
    logo: logo.png 
    toc: false
    toc-depth: 1
    toc-title: Outline
    slide-number: true
    scrollable: false
    width: 1200
    reference-location: margin
    footer: Gerko Vink @ Anton de Kom Universiteit, Paramaribo
    standalone: true
---

## Disclaimer {.smaller}
I owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.

Images are either directly linked, or generated with StableDiffusion or DALL-E. That said, there is no information in this presentation that exceeds legal use of copyright materials in academic settings, or that should not be part of the public domain. 

::: {.callout-warning}
You **may use** any and all content in this presentation - including my name - and submit it as input to generative AI tools, with the following **exception**:

- You must ensure that the content is not used for further training of the model
:::

## Slide materials and source code
::: callout-tip
# Materials
- lecture slides on Moodle
- course page: [www.gerkovink.com/sur](https://www.gerkovink.com/sur)
- source: [github.com/gerkovink/sur](https://github.com/gerkovink/sur)
:::

## Recap

Gisteren hebben we deze onderwerpen behandeld:

- Het combineren van datasets
- Groeperen en aggregeren
- Nieuwe variabelen creëren
- Filteren en sorteren van gegevens
- Het maken en aanpassen van datagroepen
- Clustering van gegevens


## Today
Vandaag behandelen we de volgende onderwerpen:

- Beschrijvende statistiek
- Kruistabellen en frequentieverdelingen
- $\chi ^2$-toets
- Andere toets- en associatiematen
- Simpele lineaire regressie
- Analyses draaien op groepen

##  We use the following packages
```{r message=FALSE}
library(dplyr) # data manipulation
library(magrittr) # for the pipe
library(psych) # descriptive statistics
library(mice) # for the boys data
library(haven) # for reading Stata files
```
```{r echo=FALSE}
library(ggplot2) # for plotting
```

# Boys data
```{r}
boys %>% 
  slice_head(n=6) # select first 6 rows
```

# Beschrijvende statistiek

## `psych::describe()`
Yesterday we already used the `describe()` function from package `psych`
```{r}
boys %>% 
  select(1:5) %>% # select first 5 columns
  psych::describe()
```

## `summary()`
The `summary()` function is a base R function that provides a summary of the data. It is a very useful function for quick inspection of variables
```{r}
boys %>% summary()
```

## `summary()` only on the non-numeric columns
To perform summary only on the non-numeric columns in the `boys` data set, we can use all of the skills we have learned so far:
```{r}
boys %>% 
  select(!where(is.numeric)) %>% # pay attention to the ! location
  summary()
```
# Contingency tables

## `table()` with 2 variables
```{r}
boys %$% 
  table(gen, phb)
```

## `table()` with 3 variables
```{r}
boys %$% 
  table(gen, phb, reg)
```

## Plotting tables
```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  plot() # reg on the x-axis and gen on the y-axis
```

## Proportional tables
```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table() # table proportions
```
<br>

```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table() %>%  # table proportions
  sum() # check that the table sums up to 1
```

## proportional tables - row margin
```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table(margin = 1) # rows sum up to 1
```
<br>

```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table(margin = 1) %>% # rows sum up
  rowSums() # check that rows sum up to 1
```

The argument `margin = 1` means that the proportions are calculated for each row, so that the rows sum up to 1. Remember that in `R` rows are always the first margin, and columns are the second margin. Just like in matrices, where the first dimension is the rows and the second dimension is the columns (`matrix(data, nrow = 3, ncol = 2)`). The same with subsetting: `data[1:3, 1:2]` means the first three rows and the first two columns.

## proportional tables - column margin
```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table(margin = 2) # columns sum up to 1 
```
<br>

```{r}
boys %$% 
  table(reg, gen) %>% # table with reg x gen
  prop.table(margin = 2) %>%  # columns sum up to 1 
  colSums() # check that columns sum up to 1
```

# Frequency distributions

## Inspecting continuous data
```{r}
boys$age %>% hist()
```

## Inspecting continuous data
```{r}
boys$age %>% 
  density() %>% 
  plot()
```

## Inspecting categorical data
```{r}
boys$gen %>% plot()
```
## Plotting data frames
```{r}
boys %>% 
  plot()
```

## Plotting (parts of) data frames
```{r}
boys %>% 
  select(where(is.numeric)) %>% 
  plot()
```

## Plotting (parts of) data frames
```{r}
boys %>% 
  select(where(is.factor)) %>% 
  plot()
```

## Plotting (parts of) data frames
```{r}
boys %>% 
  select(age, hgt, gen, phb) %>% 
  plot()
```

# Association measures

## Correlation between continuous variables
```{r}
boys %>% 
  select(where(is.numeric)) %>% # select numeric columns
  cor(use = "pairwise.complete.obs") # correlation matrix
```
## `psych::cor.plot()`
```{r}
boys %>% 
  select(where(is.numeric)) %>% 
  cor.plot()
```

## Testing the correlation
```{r}
boys %$% 
  cor.test(hgt, wgt) # correlation test between height and weight
```

## Correlations between ordered categorical variables
We can use Spearman's rank-order correlation to calculate the association between two ordered (ordinal) vectors
```{r}
boys %>% 
  select(where(is.ordered)) %>% # select ordered categorical columns
  mutate(across(where(is.ordered), ~ as.numeric(.))) %>% 
  cor(method = "spearman") # spearman correlation
```

```{r}
boys %>% 
  select(where(is.ordered)) %>% # select ordered categorical columns
  mutate(across(where(is.ordered), ~ as.numeric(.))) %>% 
  cor(use = "pairwise.complete.obs", method = "spearman") # spearman correlation
```

## Manual calculation of Spearman's $\rho$
```{r}
genphb <- boys %>% select(gen, phb) %>% na.omit() # joint observations
rx <- rank(as.numeric(genphb$gen), ties.method = "average") # rank the values of gen
ry <- rank(as.numeric(genphb$phb), ties.method = "average") # rank the values of phb
rho <- cov(rx, ry) / (sd(rx) * sd(ry))
rho
```

```{r}
boys %>% 
  select(where(is.ordered)) %>% # select ordered categorical columns
  mutate(across(where(is.ordered), ~ as.numeric(.))) %$% 
  cor.test(gen, phb, method = "spearman") # spearman correlation
```
## Ties
In the following chunk, the 2nd and 3rd values are tied
```{r}
x <- c(10, 20, 20, 40, 50)
y <- c(1, 2, 2, 4, 5)
```

Tied ranks will by default recieve the average of their ranks
```{r}
rank(x)  # returns: 1 2.5 2.5 4 5
rank(y)  # returns: 1 2.5 2.5 4 5
```

This can cause problems with the robustness of Spearman's $\rho$. 

The reason why I mention this, is that results may differ between packages or statistical processors (e.g. `R`, STATA, SPSS, etc) because the ties are by default handled differently. In such cases, explore `?rank` to see what methods are available. 

## Kendall's tau
Use Kendall’s tau for small, clean, tied, or ordinal data.
Use Spearman’s rho for quick rank-based correlation on larger datasets.
```{r}
boys %>% 
  select(where(is.ordered)) %>% # select ordered categorical columns
  mutate(across(where(is.ordered), ~ as.numeric(.))) %$% 
  cor.test(gen, phb, method = "kendall") # spearman correlation
```


# Difference tests for two groups

## Student's T-test
```{r}
boys %>% 
  mutate(overweight = case_when(bmi >= 25 ~ "overweight", 
                                bmi < 25 ~ "not overweight")) %$% 
  t.test(age ~ overweight)
```

The Welch Two Sample t-test compares the means of two groups while allowing for unequal variances and sample sizes between them. Assumptions:

	-	The data in each group are normally distributed (or approximately so),
	-	The observations are independent,
	-	But it does not assume equal variances between groups (unlike the classic Student’s t-test).

## Student's T-test
```{r}
boys %>% 
  mutate(city = case_when(reg == "city" ~ "city", 
                          reg != "city" ~ "rural")) %$% 
  t.test(bmi ~ city)
```

## $X^2$-test for 2 categories
```{r}
boys_new <- boys %>% 
  mutate(city = case_when(reg == "city" ~ "city", 
                          reg != "city" ~ "rural"),
         overweight = case_when(bmi >= 25 ~ "overweight", 
                                bmi < 25 ~ "not overweight"))

boys_new %$% table(city, overweight) 
```
## $X^2$-test for 2 categories
```{r}
boys_new %$% 
  table(city, overweight) %>% 
  chisq.test()

boys_new %$%
  chisq.test(city, overweight)
```

## Expected cell frequencies
```{r}
X2_boys <- boys_new %$%
  chisq.test(city, overweight)
X2_boys$observed
X2_boys$expected
```

We can also manually calculate the expected cell frequencies. For example, for the cell [1, 1], the cell frequency would be

```{r}
(704 * 71) / 724 # column [, 1] total times row [1, ] total divided by grand total
```

## Extract expected cell frequencies with the pipe
We can do this in the pipe by using the placeholder `.`:
```{r}
boys_new %$%
  chisq.test(city, overweight) %>% 
  .$expected
```

## Fisher exact test
If the expected cell frequencies are too low, it is more robust to calculate Fisher's exact test instead of the $\chi^2$-test. 
```{r}
boys_new %$%
  fisher.test(city, overweight)
```

Fisher’s exact test is better than the chi-squared test for low expected cell frequencies because it calculates the exact p-value without relying on large-sample approximations, making it more accurate when expected counts are small.

# Difference tests for more than two groups

## $X^2$-test 
```{r}
boys %$% 
  table(gen, phb)
```
```{r}
boys %$% 
  chisq.test(gen, phb)
```

## $X^2$-test or Fisher exact test
```{r}
boys %$% 
  chisq.test(gen, phb) %>% 
  .$expected
```
```{r eval=FALSE}
boys %$%
  fisher.test(gen, phb)

Error in fisher.test(gen, phb) : 
  FEXACT error 6 (f5xact).  LDKEY=621 is too small for this problem: kval=186370998.
Try increasing the size of the workspace.
```

In this case the p-value is so small, that a lot of memory is needed to model exactly how small the p-value is. 

## Simulate Fisher exact test
```{r}
boys %$%
  fisher.test(gen, phb, simulate.p.value = TRUE)
```

```{r}
boys %$%
  fisher.test(gen, phb, 
              simulate.p.value = TRUE, 
              B = 100000) # number of monte carlo simulations
```

## One-way ANOVA
```{r}
boys %>% 
  group_by(reg) %>% 
  summarise(mean_age = mean(age, na.rm=TRUE))
boys %$% 
  lm(age ~ reg) %>% 
  anova()
```
## One-way ANOVA
```{r}
boys %$% 
  lm(age ~ reg) %>% 
  summary()
```

## `group_by()` and analyses
```{r}
boys %>%
  group_by(reg) %>%
  summarise(correlation = cor(hgt, wgt, use = "pairwise.complete.obs"))
```
NOTE: `summarise()` expects outputs that are scalars, not lists or objects.

## `group_by()` and complex analyses
You can’t directly use `summarise()` with complex analyses like `cor.test()` to extract output because:

	- cor.test() returns a complex object (not just a number).
	
There are ways around this
```{r}
library(purrr)
boys%>%
  group_by(reg) %>%
  summarise(
    test = list(cor.test(hgt, wgt)),
    estimate = map_dbl(test, ~ .x$estimate),
    p_value = map_dbl(test, ~ .x$p.value)
  )
```
We will explore these ways in the next lecture. 

## For fun
<center> 
![](img/lec-4/going_out_for_a_byte.png){width=40%}
<br>[source](https://www.reddit.com/r/ProgrammerHumor/comments/8za9b6/hello_i_am_your_server_for_today/)
</center>

# Practical
