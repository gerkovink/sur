---
title: "Data Visualization"
subtitle: ".. and a fair bit of statistical learning"
author: 
  - name: Gerko Vink
    orcid: 0000-0001-9767-1924
    email: g.vink@uu.nl
    affiliations:
      - name: Methodology & Statistics @ Utrecht University
date: 2 June 2025
date-format: "D MMM YYYY"
execute: 
  echo: true
format: 
  revealjs:
    theme: [solarized, gerko.scss]
    embed-resources: true
    progress: true
    margin: 0.075
    logo: logo.png 
    toc: false
    toc-depth: 1
    toc-title: Outline
    slide-number: true
    scrollable: false
    width: 1200
    reference-location: margin
    footer: Gerko Vink @ Anton de Kom Universiteit, Paramaribo
    standalone: true
---

## Disclaimer {.smaller}
I owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.

These materials are generated by Gerko Vink, who holds the copyright. The intellectual property belongs to Utrecht University. Images are either directly linked, or generated with StableDiffusion or DALL-E. That said, there is no information in this presentation that exceeds legal use of copyright materials in academic settings, or that should not be part of the public domain. 

::: {.callout-warning}
You **may use** any and all content in this presentation - including my name - and submit it as input to generative AI tools, with the following **exception**:

- You must ensure that the content is not used for further training of the model
:::

## Slide materials and source code
::: callout-tip
# Materials
- course page: [www.gerkovink.com/sur](https://www.gerkovink.com/sur)
- source: [github.com/gerkovink/sur](https://github.com/gerkovink/sur)
:::

## Recap

Yesterday we have learned:

## Today
Today we will learn how to:


## This lecture

## Packages and functions that we use
```{r message=FALSE, warning=FALSE}
library(dplyr)    # Data manipulation
library(magrittr) # Pipes
library(ggplot2)  # Plotting suite
library(MASS)     # Dataset
library(class)    # K-nearest Neighbour
library(mvtnorm)  # Multivariate Normal tools
```

### Custom theme for plots
```{r}
helpIAmColourblind <- scale_color_manual(values = c("orange", 
                                                    "blue", 
                                                    "dark green"))
```


# What is statistical learning?

## Statistics is everywhere

Several questions involving statistics:

1. What is the relation between $X$ and $Y$? (estimation)
2. What is the uncertainty around this effect? (estimation/inference)
3. What can I conclude about my population? (inference/hypothesis testing)

4. How can I best predict new observations? (prediction)
5. How can I show relevant patterns in my data? (dimension reduction / pattern recognition)

## Examples
- Radiologists use statistics to sharpen their images (e.g. MRI) and improve their diagnosis.
- Doctors use statistics to target your treatment to your symptoms or body.
- Physicists use statistics to find useful patterns in the huge data dumps by the Large Hadron Collider.
- Insurance companies use statistics to model risks for potential clients.
- Google uses statistics to serve you targeted ads.
- Netflix uses statistics to create hit shows.
- Spotify uses statistics to suggest music to you.

## Supervised learning
In supervised learning we aim to quantify the relation between $Y$ and $X$.

$Y$:

- target
- outcome
- dependent
- response

$X$:

- features
- predictors
- independent
- input

## Supervised learning
We want to find the predictive function:

$$Y = f(X) + \epsilon $$

That minimizes $\epsilon$ with respect to our goal.

- Function $f$ is an unknown, but fixed function of $X = X1, \dots, Xp$
- $p$ is the number of predictors
- $Y$ is the quantitative response 
- $\epsilon \sim N(0, \sigma_\epsilon^2)$ is a random error term
- $\epsilon$ does not depend on $X$

Our aim is to find the $f(X)$ that best represent the systematic information that $X$ yields about $Y$. 

## Supervised learning
With *supervised learning* every observation on our predictor 

$$x_i, i=1, \dots, n$$ 

has a corresponding outcome measurement 

$$y_i$$
such that

$$\hat{y_i}=f({\bf x_i})\quad \text{and} \quad  y_i = f({\bf x_i})+\epsilon_i.$$

Examples:

- linear regression
- logistic regression
- k-nearest neighbours classifying

## Unsupervised learning
With *unsupervised learning* we have a vector of measurement $\bf x_i$ for every unit $i=1, \dots, n$, but we miss the associated response $y_i$. 


1. There is no outcome to predict
  
    - Hence you cannot fit e.g. a linear regression model
 
2. There is no outcome to verify the model
  
    - We lack the *truth* to supervise our analysis

## What can we do?

Find patterns in $\bf x_1, \dots, x_n$

We can use this model to e.g. find out if some cases are more similar than other cases or which variables explain most of the variation

Examples:

- Principal Components Analysis
- k-means clustering

# K-means and K-nearest neighbours

## Two nonparametric algorithms

### K-nearest neighbours (KNN)

- supervised learning
- prediction
- classification
 
### K-means clustering (kmeans)

- unsupervised learning
- dimension reduction / pattern recognition
- clustering

## Example dataset
Let's create some data from a multivariate normal distribution

We start with fixing the random seed
```{r}
set.seed(123)
```

and specifying the variance covariance matrix:
```{r}
sigma <- matrix(c(1, .5, .5, 1), 2, 2)
rownames(sigma) <- colnames(sigma) <- c("x1", "x2")
```

## Data relations
```{r}
sigma
```

Because the variances are `1`, the resulting data will have a correlation of $$\rho = \frac{\text{cov}(y, x)}{\sigma_y\sigma_x} = \frac{.5}{1\times1} = .5.$$

Let's draw the data
```{r}
sim.data <- mvtnorm::rmvnorm(n     = 100, 
                             mean  = c(5, 5), 
                             sigma = sigma)
colnames(sim.data) <- c("x1", "x2")
```

## Plot the data
```{r, fig.height=4}
sim.data %>% 
  as_tibble %>%
  ggplot(aes(x1, x2)) +
  geom_point()
```

## Now add some clustering
```{r}
sim.data <- 
  sim.data %>%
  as_tibble %>%
  mutate(class = sample(c("A", "B", "C"), size = 100, replace = TRUE))
```
We have added a new column that randomly assigns rows to level `A`, `B` or `C`
```{r}
sim.data %>% head
```

## Plot the data again
```{r, fig.height=4}
sim.data %>%
  ggplot(aes(x1, x2,  colour = class)) +
  geom_point() + 
  helpIAmColourblind
```

## Adjust the clusters to make them distinct
```{r}
sim.data <- 
  sim.data %>%
  mutate(x2 = case_when(class == "A" ~ x2 + 1.5,
                        class == "B" ~ x2 - 1.5,
                        class == "C" ~ x2 + 1.5),
         x1 = case_when(class == "A" ~ x1 - 1.5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + 1.5))
```


## The result: supervised
```{r, fig.height=4}
sim.data %>%
  ggplot(aes(x1, x2,  colour = class)) +
  geom_point() + 
  helpIAmColourblind
```

## The result: unsupervised
```{r, fig.height=4}
sim.data %>%
  ggplot(aes(x1, x2)) +
  geom_point()
```

# K-Nearest Neighbors

## How does it work?

1. For every test observation $x_0$ the $K$ points that are close to $x_0$ are identified.
2. These *closest* points form set $\mathcal{N}_0$.
3. We estimate the probability for $x_0$ being part of class $j$ as the fraction of points in $\mathcal{N}_0$ for whom the response equals $j$:
$$P(Y = j | X = x_0) = \frac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i=j)$$

4. The observation $x_0$ is classified to the class with the largest probability

### In short
An observation gets that class assigned to which most of its $K$ neighbours belong

## Why KNN?

Because $X$ is assigned to the class to which most of the observations belong it is

- non-parametric

  - no assumptions about the distributions, or the shape of the decision boundary

- expected to be far better than logistic regression when decision boundaries are non-linear

However, we do not get parameters as with LDA and regression. 

- We thus cannot determine the relative importance of predictors 
- The "model" == the existing observations: instance-based learning
 
## Fitting a K-NN model
First we need to determine a training set
```{r}
set.seed(123)
sim.data <-
  sim.data %>% 
  mutate(set = sample(c("Train", "Test"), size = 100, 
                      prob = c(.25, .75), replace = TRUE))
sim.data
```

## Fitting a K-NN model
Then we split the data into a training (build the model) and a test (verify the model) set
```{r}
train.data <- subset(sim.data, set == "Train", select = c(x1, x2))
test.data <-  subset(sim.data, set == "Test",  select = c(x1, x2))
obs.class <-  subset(sim.data, set == "Train", select = class)
```

Now we can fit the K-NN model
```{r}
fit.knn <- knn(train = train.data,
               test  = test.data, 
               cl    = as.matrix(obs.class),
               k     = 3)
fit.knn
```

## Predictions
```{r}
class.test <- subset(sim.data, set == "Test", select = class) %>%
  as.matrix()
correct <- fit.knn == class.test
mean(correct)
table(fit.knn, class.test)
```

## The (in)correct responses KNN = 3
```{r, fig.height=4}
cbind(test.data, correct) %>%
  ggplot(aes(x1, x2,  colour = correct)) +
  geom_point() +
  scale_colour_manual(values = c("red", "black"))
```

## Fewer neighbours
```{r}
fit.knn <- knn(train = train.data,
               test  = test.data, 
               cl    = as.matrix(obs.class),
               k     = 2)
correct <- fit.knn == class.test
mean(correct)
table(fit.knn, class.test)
```

## More neighbours
```{r}
fit.knn <- knn(train = train.data,
               test  = test.data, 
               cl    = as.matrix(obs.class),
               k     = 4)
correct <- fit.knn == class.test
mean(correct)
table(fit.knn, class.test)
```

## Even more neighbours
```{r}
fit.knn <- knn(train = train.data,
               test = test.data, 
               cl = as.matrix(obs.class),
               k = 10)
correct <- fit.knn == class.test
mean(correct)
table(fit.knn, class.test)
```

## The (in)correct responses KNN = 10
```{r, fig.height=4}
cbind(test.data, correct) %>%
  ggplot(aes(x1, x2,  colour = correct)) +
  geom_point() +
  scale_colour_manual(values = c("red", "black"))
```

## Predicting a new observation

Let's make a new observation:

```{r}
newObs <- data.frame(x1 = 5.5, x2 = 4.5)
```

```{r, echo=FALSE}
sim.data %>%
  ggplot() +
  geom_point(aes(x1, x2,  colour = class)) + 
  geom_point(data = newObs, aes(x1, x2), size = 4, col = "black") +
  helpIAmColourblind
```

## Predicting a new observation

```{r, echo=FALSE}
sim.data %>%
  ggplot() +
  geom_point(aes(x1, x2,  colour = class)) + 
  geom_point(data = newObs, aes(x1, x2), size = 4, col = "black") +
  geom_point(data = newObs, aes(x1, x2), size = 45, pch = 21, bg = "transparent") +
  helpIAmColourblind
```

## Predicting a new observation

Now we predict the class of this new observation, using the entire data for training our model
```{r}
knn(train = sim.data[, 1:2], cl = sim.data$class, k = 10, test = newObs)
```

# K-means clustering

## Remember: unsupervised

```{r, fig.height=4}
sim.data %>%
  ggplot(aes(x1, x2)) +
  geom_point()
```

## Goal: finding clusters in our data
K-means clustering partitions our dataset into $K$ distinct, non-overlapping clusters or subgroups.

## What is a cluster?
A set of _relatively similar_ observations.

### What is "relatively similar"?
This is up to the programmer/researcher to decide. For example, we can say the "within-class" variance is as small as possible and the between-class variance as large as possible. 

## Why perform clustering? 

We expect clusters in our data, but weren't able to measure them

- potential new subtypes of cancer tissue

We want to summarise features into a categorical variable to use in further decisions/analysis

- subgrouping people by their spending types

## The k-means algorithm

1. Randomly assign values to K classes
2. Calculate the centroid (`colMeans`) for each class
3. Assign each value to its closest centroid class
4. If the assignments changed, go to step 2. else stop.

## 

<img src="img/lec-7/kmeans.png" style="display:block;margin:0 auto;margin-top:-10%;width:70%"></img>

<p align="center" style="font-style:italic;">Source: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). New York: Springer.</p>

## The k-means algorithm

K is a __tuning parameter__ (centers)

```{r}
(fitkm <- kmeans(sim.data[, 1:2], centers = 3))
```

## The result:
```{r, fig.align='center', fig.height=4}
sim.data$clust <- as.factor(fitkm$cluster)

sim.data %>% ggplot +
  geom_point(aes(x = x1, y = x2, colour = clust)) +
  helpIAmColourblind
```

## Comparison

```{r, fig.align='center', fig.height=4}
# this is the data-generating class

sim.data %>% ggplot +
  geom_point(aes(x = x1, y = x2, colour = class)) +
  helpIAmColourblind
```

## Centroids
```{r, fig.align='center', fig.height=4}
sim.data %>% ggplot +
  geom_point(aes(x = x1, y = x2, colour = clust)) +
  geom_point(aes(x = x1, y = x2), data = as.data.frame(fitkm$centers),
             size = 5, col = "red", alpha = 0.8) +
  helpIAmColourblind
```

## K = 5
```{r, echo=FALSE, fig.align='center'}
sim.data$clust <- as.factor(kmeans(sim.data[,1:2], 5)$cluster)

sim.data %>% ggplot +
  geom_point(aes(x = x1, y = x2, colour = clust)) +
  scale_colour_manual(values = viridisLite::viridis(5))
```

## K = 2
```{r, echo=FALSE, fig.align='center'}
sim.data$clust <- as.factor(kmeans(sim.data[,1:2], 2)$cluster)

sim.data %>% ggplot +
  geom_point(aes(x = x1, y = x2, colour = clust)) +
  helpIAmColourblind
```

## Conclusion

- Supervised learning: outcome / target available
- Unsupervised learning: no outcome / target
- prediction & pattern recognition vs. estimation, inference, testing
- knn: nonparametric classification
- kmeans: clustering algorithm

- With the power of `R` we can generate any data we want and know the 'truth'!

## We use the following packages
```{r, message=FALSE, warning=FALSE}
library(MASS)     # Datasets
library(mice)     # Boys dataset
library(dplyr)    # Data manipulation
library(magrittr) # Pipes
library(ggplot2)  # Plotting suite
```

##

New functions:

- `hist()`: histogram
- `plot()`: R's plotting device
- `barplot()`: bar plot function
- `boxplot()`: box plot function
- `density()`: function that calculates the density
- `ggplot()`: ggplot's plotting device


## Why visualise?

- We can process a lot of information quickly with our eyes
- Plots give us information about
    - Distribution / shape
    - Irregularities
    - Assumptions
    - Intuitions
- Summary statistics, correlations, parameters, model tests, *p*-values do not tell the whole story

### ALWAYS plot your data!


## Why visualise?

<img src="img/lec-7/anscombe.svg" style="display:block;width:90%;margin:0 auto;"></img>
<p style="text-align:center;font-style:italic;font-size:0.5em;">Source: Anscombe, F. J. (1973). "Graphs in Statistical Analysis". American Statistician. 27 (1): 17â€“21.</p>


## Why visualise?

<img src="img/lec-7/datasaurus.gif" style="display:block;width:90%;margin:0 auto;"></img>


<p style="text-align:center;font-style:italic;font-size:0.5em;">Source: https://www.autodeskresearch.com/publications/samestats</p>

## What we will do

- A few plots in `base` graphics in `R`
- Plotting with `ggplot2` graphics

## Plots

```{r, echo=FALSE, fig.align='center'}
par(mfrow = c(2,2), mar = c(4, 4, 3, 1))
boys %$% hist(na.omit(hgt), main = "Histogram", xlab = "Height")
boys %$% plot(density(na.omit(hgt)), main = "Density plot", xlab = "Height", bty = "L")
boys %$% plot(hgt, wgt, main = "Scatter plot", xlab = "Height", ylab = "Weight", bty = "L")
boys %$% boxplot(hgt~reg, main = "Boxplot", xlab = "Region", ylab = "Height")
``` 

## Histogram
```{r, fig.align='center', dev.args=list(bg="transparent")}
hist(boys$hgt, main = "Histogram", xlab = "Height")
```

## Density
```{r, fig.align='center', dev.args=list(bg="transparent")}
dens <- density(boys$hgt, na.rm = TRUE)
plot(dens, main = "Density plot", xlab = "Height", bty = "L")
```

## Scatter plot
```{r, fig.align='center', dev.args=list(bg="transparent")}
plot(x = boys$hgt, y = boys$wgt, main = "Scatter plot", 
     xlab = "Height", ylab = "Weight", bty = "L")
```


## Box plot
```{r, fig.align='center', dev.args=list(bg="transparent")}
boxplot(boys$hgt ~ boys$reg, main = "Boxplot", 
        xlab = "Region", ylab = "Height")
```

<!-- ## 3d plots -->
<!-- ```{r, fig.align='center', dev.args=list(bg="transparent")} -->
<!-- persp(volcano, theta = 120) -->
<!-- ``` -->

## Box plot II

```{r, fig.align='center', dev.args=list(bg="transparent")}
boxplot(hgt ~ reg, boys,  main = "Boxplot", xlab = "Region", 
        ylab = "Height", lwd = 2, notch = TRUE, col = rainbow(5))
```

## A lot can be done in base R!

```{r dev.args=list(bg="transparent")}
boys %>% md.pattern() # from mice
```

## Many R objects have a `plot()` method

```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
result <- lm(age~wgt, boys)
plot(result, which = 1)
```


## Neat! But what if we want more control?

# ggplot2

## What is `ggplot2`?
Layered plotting based on the book **The Grammer of Graphics** by Leland Wilkinsons.

With `ggplot2` you

1. provide the _data_
2. define how to map variables to _aesthetics_
3. state which _geometric object_ to display
4. (optional) edit the overall _theme_ of the plot

`ggplot2` then takes care of the details

## An example: scatterplot

1: Provide the data
```{r, eval=FALSE}
boys %>%
  ggplot()
```

2: map variable to aesthetics
```{r, eval=FALSE}
boys %>%
  ggplot(aes(x = age, y = bmi))
```

3: state which geometric object to display
```{r, eval=FALSE}
boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point()
```

## An example: scatterplot
```{r, echo=FALSE, fig.align='center'}
boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point(na.rm = TRUE)
```

## Why this syntax?

Create the plot
```{r, fig.align='center', dev.args=list(bg="transparent"), warning=FALSE, message=FALSE}
gg <- 
  boys %>%
  ggplot(aes(x = age, y = bmi)) +
  geom_point(col = "dark green")
```

Add another layer (smooth fit line)
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
gg <- gg + 
  geom_smooth(col = "dark blue")
```

Give it some labels and a nice look
```{r, fig.align='center', dev.args=list(bg="transparent")}
gg <- gg + 
  labs(x = "Age", y = "BMI", title = "BMI trend for boys") +
  theme_minimal()
```

## Why this syntax?
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
plot(gg)
```

## Why this syntax?
<img src="img/lec-7/ggani.gif" style="display:block;width:90%;margin:0 auto;"></img>

## Aesthetics

- x
- y
- size
- colour
- fill
- opacity (alpha)
- linetype
- ...

## Aesthetics
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
gg <- 
  boys %>% 
  filter(!is.na(reg)) %>% 
  
  ggplot(aes(x      = age, 
             y      = bmi, 
             size   = hc, 
             colour = reg)) +
  
  geom_point(alpha = 0.5) +
  
  labs(title  = "BMI trend for boys",
       x      = "Age", 
       y      = "BMI", 
       size   = "Head circumference",
       colour = "Region") +
  theme_minimal()
```
 
## Aesthetics
```{r, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
plot(gg)
```

## Geoms

- geom_point
- geom_bar
- geom_line
- geom_smooth

- geom_histogram
- geom_boxplot
- geom_density

## Geoms: Bar
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
data.frame(x = letters[1:5], y = c(1, 3, 3, 2, 1)) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_bar(fill = "dark green", stat = "identity") +
  labs(title = "Value per letter",
       x     = "Letter", 
       y     = "Value") +
  theme_minimal()
```

## Geoms: Line
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
ggdat <- data.frame(x = 1:100, y = rnorm(100))
ggdat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_line(colour = "dark green", lwd = 1) +
  ylim(-2, 3.5) +
  labs(title = "Some line thing",
       x     = "Time since start", 
       y     = "Some value") +
  theme_minimal()
```

## Geoms: Smooth
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
ggdat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(colour = "dark green", lwd = 1, se = FALSE) +
  ylim(-2, 3.5) +
  labs(title = "Some line thing",
       x     = "Time since start", 
       y     = "Some value") +
  theme_minimal()
```

## Geoms: Boxplot
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
boys %>% 
  filter(!is.na(reg)) %>% 
  
  ggplot(aes(x = reg, y = bmi, fill = reg)) +
  
  geom_boxplot() +
  
  labs(title = "BMI across regions",
       x     = "Region", 
       y     = "BMI") +
  theme_minimal() + 
  theme(legend.position = "none")
```

## Geoms: Density
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', dev.args=list(bg="transparent")}
boys %>% 
  filter(!is.na(reg)) %>% 
  ggplot(aes(x = hgt, fill = reg)) +
  geom_density(alpha = 0.5, colour = "transparent") +
  xlim(0, 250) + 
  labs(title = "Height across regions",
       x     = "Height", 
       fill  = "Region") +
  theme_minimal()
```

## Changing the Style: Themes

- Themes determine the overall appearance of your plot
- standard themes: e.g., `theme_minimal()`, `theme_classic()`, `theme_bw()`, ...
- extra libraries with additional themes: e.g., `ggthemes`
- customize own theme using options of `theme()`

## Changing the Style: Themes

```{r, echo=FALSE, warning=FALSE, message=FALSE, dev.args=list(bg="transparent"), out.width="50%", fig.show="hold"}
boys %>% 
  filter(!is.na(reg)) %>% 
  ggplot(aes(x = hgt, fill = reg)) +
  geom_density(alpha = 0.5, colour = "transparent") +
  xlim(0, 250) + 
  labs(title = "Height across regions",
       x     = "Height", 
       fill  = "Region") +
  theme_minimal()

boys %>% 
  filter(!is.na(reg)) %>% 
  ggplot(aes(x = hgt, fill = reg)) +
  geom_density(alpha = 0.5, colour = "transparent") +
  xlim(0, 250) + 
  labs(title = "Height across regions",
       x     = "Height", 
       fill  = "Region") +
  theme_gray()

boys %>% 
  filter(!is.na(reg)) %>% 
  ggplot(aes(x = hgt, fill = reg)) +
  geom_density(alpha = 0.5, colour = "transparent") +
  xlim(0, 250) + 
  labs(title = "Height across regions",
       x     = "Height", 
       fill  = "Region") +
  theme_classic()

boys %>% 
  filter(!is.na(reg)) %>% 
  ggplot(aes(x = hgt, fill = reg)) +
  geom_density(alpha = 0.5, colour = "transparent") +
  xlim(0, 250) + 
  labs(title = "Height across regions",
       x     = "Height", 
       fill  = "Region") +
  theme_dark()
```


## Helpful link in RStudio
<img src="img/lec-7/cheatsheet.png" style="display:block;width:90%;margin:0 auto;"></img>

# Practical
